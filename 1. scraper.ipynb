{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from time import gmtime, strftime\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "# Importing libraries you need to install\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import bs4 as bs\n",
    "from lxml import html\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import time as t_time\n",
    "# from ratelimit import limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luckywang/Documents/Document/Course Material/Fall 2021/esg_nlp/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data\"\n",
    "\n",
    "# read the ticker library of all the tikers into ticker_library\n",
    "ticker_library = pd.read_csv(os.path.join(data_dir, \"tickers.csv\"))\n",
    "\n",
    "# read the sp500 components into ticker_selected, 'name' is the company name and ticker is company's ticker\n",
    "ticker_selected = pd.read_csv(os.path.join(data_dir, \"sp500_component_stocks.csv\"), header = None)\n",
    "ticker_selected.columns = [\"name\", \"ticker\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a cik&ticker dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001090872</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000006201</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001158449</td>\n",
       "      <td>AAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000320193</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001551152</td>\n",
       "      <td>ABBV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cik ticker\n",
       "0  0001090872      A\n",
       "1  0000006201    AAL\n",
       "2  0001158449    AAP\n",
       "3  0000320193   AAPL\n",
       "4  0001551152   ABBV"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a ticker_cik_df dataframe to store ticker and its cik number\n",
    "ticker_cik_df = pd.DataFrame()\n",
    "\n",
    "# store all the tickers in a ticker_list\n",
    "ticker_list = ticker_selected.ticker\n",
    "\n",
    "# build a list cik_list for cik\n",
    "cik_list = []\n",
    "\n",
    "for ticker in ticker_list:    \n",
    "    try:\n",
    "        # for a given ticker, find its cik number through th ticker library\n",
    "        cik_list.append(list(ticker_library[ticker_library.ticker == ticker].secfilings)[0][-10:])\n",
    "        \n",
    "    except:\n",
    "        # if could not find cik, give it a empty cik\n",
    "        cik_list.append('')\n",
    "\n",
    "# write cik_list and ticker_list to the dataframe ticker_cik_df\n",
    "ticker_cik_df['cik'] = cik_list\n",
    "ticker_cik_df['ticker'] = ticker_list\n",
    "\n",
    "# delete the tickers with empty cik number\n",
    "ticker_cik_df = ticker_cik_df[ticker_cik_df['cik'] != '']\n",
    "\n",
    "# display a sample of ticker_cik_df\n",
    "ticker_cik_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "listtickers = ['AMZN','BBY','BKNG','MCD','EBAY','F','HD','TGT','WHR','JPM','SIVB','CFG','C','ALL','IVZ','ETFC','MET','PFG','CBOE',\n",
    "              'CTL','IPG','VIAC','NFLX','CHTR','FB','TWTR','NWSA','FOXA','AMD','INTC','AAPL','LRCX','MSFT','NLOK','CTSH','ADS',\n",
    "              'WU','PAYC','ABT','CVS','PFE','JNJ','BIIB','INCY','HSIC','WAT','ALGN','EW']\n",
    "\n",
    "ticker_cik_sample = pd.DataFrame()\n",
    "\n",
    "for ticker in listtickers:\n",
    "    ticker_cik_sample = ticker_cik_sample.append(ticker_cik_df[ticker_cik_df['ticker'] == ticker])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#F or demostration purposes we will only scrape two companies\n",
    "# ticker_cik_sample = ticker_cik_df.sample(n=2, replace=False, random_state=0)\n",
    "len(ticker_cik_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import function to scrape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WriteLogFile(log_file_name, text):\n",
    "    '''\n",
    "    Helper function.\n",
    "    Writes a log file with all notes and\n",
    "    error messages from a scraping \"session\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    log_file_name : str\n",
    "        Name of the log file (should be a .txt file).\n",
    "    text : str\n",
    "        Text to write to the log file.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    with open(log_file_name, \"a+\") as log_file:\n",
    "        log_file.write(text)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Safari/605.1.15 Version/13.0.4\",\n",
    "          \"referer\": \"http://localhost:8888/\",\n",
    "          \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_2 = {\"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\",\n",
    "          \"referer\": \"http://localhost:8888/\",\n",
    "          \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape10K(browse_url_base, filing_url_base, doc_url_base, cik, log_file_name):\n",
    "    \n",
    "    '''\n",
    "    Scrapes all 10-Ks and 10-K405s for a particular \n",
    "    CIK from EDGAR.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    browse_url_base : str\n",
    "        Base URL for browsing EDGAR.\n",
    "    filing_url_base : str\n",
    "        Base URL for filings listings on EDGAR.\n",
    "    doc_url_base : str\n",
    "        Base URL for one filing's document tables\n",
    "        page on EDGAR.\n",
    "    cik : str\n",
    "        Central Index Key.\n",
    "    log_file_name : str\n",
    "        Name of the log file (should be a .txt file).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    data_dir = os.path.join(\"data\", \"10k\", cik)\n",
    "    # Check if we've already scraped this CIK\n",
    "    try:\n",
    "        os.mkdir(data_dir)\n",
    "    except OSError:\n",
    "        print(\"Already scraped CIK\", cik)\n",
    "        return\n",
    "    \n",
    "    print('Scraping CIK', cik)\n",
    "    \n",
    "    # Request list of 10-K filings\n",
    "    res = requests.get(browse_url_base % cik, headers=headers)\n",
    "    \n",
    "    # If the request failed, log the failure and exit\n",
    "    if res.status_code != 200:\n",
    "        os.rmdir(data_dir) # remove empty dir\n",
    "        text = \"Request failed with error code \" + str(res.status_code) + \\\n",
    "               \"\\nFailed URL: \" + (browse_url_base % cik) + '\\n'\n",
    "        print(\"main\", text)\n",
    "        WriteLogFile(log_file_name, text)\n",
    "        return\n",
    "    else:\n",
    "        WriteLogFile(log_file_name, \"Success URL: {}\\n\".format(browse_url_base % cik))\n",
    "\n",
    "    # If the request doesn't fail, continue...\n",
    "    \n",
    "    # Parse the response HTML using BeautifulSoup\n",
    "    soup = bs.BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    "    # Extract all tables from the response\n",
    "    html_tables = soup.find_all('table')\n",
    "    \n",
    "    # Check that the table we're looking for exists\n",
    "    # If it doesn't, exit\n",
    "    if len(html_tables) < 3:\n",
    "        return\n",
    "    \n",
    "    # Parse the Filings table\n",
    "    filings_table = pd.read_html(str(html_tables[2]), header=0)[0]\n",
    "    filings_table['Filings'] = [str(x) for x in filings_table['Filings']]\n",
    "\n",
    "    # Get only 10-K and 10-K405 document filings\n",
    "    filings_table = filings_table[(filings_table['Filings'] == '10-K') | (filings_table['Filings'] == '10-K405')]\n",
    "\n",
    "    # If filings table doesn't have any\n",
    "    # 10-Ks or 10-K405s, exit\n",
    "    if len(filings_table) == 0:\n",
    "        return\n",
    "    \n",
    "    # Get accession number for each 10-K and 10-K405 filing\n",
    "    filings_table['Acc_No'] = [x.replace('\\xa0',' ')\n",
    "                               .split('Acc-no: ')[1]\n",
    "                               .split(' ')[0] for x in filings_table['Description']]\n",
    "\n",
    "    # Iterate through each filing and \n",
    "    # scrape the corresponding document...\n",
    "    for index, row in filings_table.iterrows():\n",
    "        if row[\"Filing Date\"] < \"2020-01-01\":\n",
    "            continue\n",
    "        \n",
    "        # Get the accession number for the filing\n",
    "        acc_no = str(row['Acc_No'])\n",
    "        \n",
    "        # Navigate to the page for the filing\n",
    "        docs_page = requests.get(filing_url_base % (cik, acc_no), headers=headers_2)\n",
    "        \n",
    "        # If request fails, log the failure\n",
    "        # and skip to the next filing\n",
    "        if docs_page.status_code != 200:\n",
    "            text = \"Request failed with error code \" + str(docs_page.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (filing_url_base % (cik, acc_no)) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            continue\n",
    "        else:\n",
    "            WriteLogFile(log_file_name, \"Success URL: {}\\n\".format(filing_url_base % (cik, acc_no)))\n",
    "\n",
    "        # If request succeeds, keep going...\n",
    "        \n",
    "        # Parse the table of documents for the filing\n",
    "        docs_page_soup = bs.BeautifulSoup(docs_page.text, 'lxml')\n",
    "        docs_html_tables = docs_page_soup.find_all('table')\n",
    "        if len(docs_html_tables)==0:\n",
    "            continue\n",
    "        docs_table = pd.read_html(str(docs_html_tables[0]), header=0)[0]\n",
    "        docs_table['Type'] = [str(x) for x in docs_table['Type']]\n",
    "        \n",
    "        # Get the 10-K and 10-K405 entries for the filing\n",
    "        docs_table = docs_table[(docs_table['Type'] == '10-K') | (docs_table['Type'] == '10-K405')]\n",
    "        \n",
    "        # If there aren't any 10-K or 10-K405 entries,\n",
    "        # skip to the next filing\n",
    "        if len(docs_table)==0:\n",
    "            continue\n",
    "        # If there are 10-K or 10-K405 entries,\n",
    "        # grab the first document\n",
    "        elif len(docs_table)>0:\n",
    "            docs_table = docs_table.iloc[0]\n",
    "        \n",
    "        docname = docs_table['Document']\n",
    "        \n",
    "        # If that first entry is unavailable,\n",
    "        # log the failure and exit\n",
    "        if str(docname) == 'nan':\n",
    "            text = 'File with CIK: %s and Acc_No: %s is unavailable' % (cik, acc_no) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            continue       \n",
    "        \n",
    "        # If it is available, continue...\n",
    "        docname = docname.split(' ')[0]\n",
    "        \n",
    "        # Request the file\n",
    "        file = requests.get(doc_url_base % (cik, acc_no.replace('-', ''), docname), headers=headers)\n",
    "        \n",
    "        # If the request fails, log the failure and exit\n",
    "        if file.status_code != 200:\n",
    "            text = \"Request failed with error code \" + str(file.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (doc_url_base % (cik, acc_no.replace('-', ''), docname)) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            continue\n",
    "        else:\n",
    "            WriteLogFile(log_file_name, \"Success URL: {}\\n\".format(doc_url_base % (cik, acc_no.replace('-', ''), docname)))\n",
    "        \n",
    "        # If it succeeds, keep going...\n",
    "        \n",
    "        # Save the file in appropriate format\n",
    "        \"\"\"\n",
    "        date = str(row['Filing Date'])\n",
    "        filename = cik + '_' + date + '.txt'\n",
    "        html_file = open(filename, 'a')\n",
    "        html_file.write(file.text)\n",
    "        html_file.close()\n",
    "        \"\"\"\n",
    "        if '.txt' in docname:\n",
    "            # Save text as TXT\n",
    "            date = str(row['Filing Date'])\n",
    "            filename = cik + '_' + date + '.txt'\n",
    "            file_path = os.path.join(\"data\", \"10k\", cik, filename)\n",
    "            html_file = open(file_path, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "        else:\n",
    "            # Save text as HTML\n",
    "            date = str(row['Filing Date'])\n",
    "            filename = cik + '_' + date + '.html'\n",
    "            file_path = os.path.join(\"data\", \"10k\", cik, filename)\n",
    "            html_file = open(file_path, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "        \n",
    "#     # Move back to the main 10-K directory\n",
    "#     os.chdir('..')\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape10Q(browse_url_base, filing_url_base, doc_url_base, cik, log_file_name):\n",
    "    \n",
    "    '''\n",
    "    Scrapes all 10-Qs for a particular CIK from EDGAR.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    browse_url_base : str\n",
    "        Base URL for browsing EDGAR.\n",
    "    filing_url_base : str\n",
    "        Base URL for filings listings on EDGAR.\n",
    "    doc_url_base : str\n",
    "        Base URL for one filing's document tables\n",
    "        page on EDGAR.\n",
    "    cik : str\n",
    "        Central Index Key.\n",
    "    log_file_name : str\n",
    "        Name of the log file (should be a .txt file).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Check if we've already scraped this CIK\n",
    "    data_dir = os.path.join(\"data\", \"10q\", cik)\n",
    "    try:\n",
    "        os.mkdir(data_dir)\n",
    "    except OSError:\n",
    "        print(\"Already scraped CIK\", cik)\n",
    "        return\n",
    "    \n",
    "    # If we haven't, go into the directory for that CIK\n",
    "    \n",
    "    #I avoid print here\n",
    "    \n",
    "    print('Scraping CIK', cik)\n",
    "    \n",
    "    # Request list of 10-Q filings\n",
    "    res = requests.get(browse_url_base % cik, headers=headers)\n",
    "    \n",
    "    # If the request failed, log the failure and exit\n",
    "    if res.status_code != 200:\n",
    "        os.rmdir(data_dir) # remove empty dir\n",
    "        text = \"Request failed with error code \" + str(res.status_code) + \\\n",
    "               \"\\nFailed URL: \" + (browse_url_base % cik) + '\\n'\n",
    "        print(\"main\", text)\n",
    "        WriteLogFile(log_file_name, text)\n",
    "        return\n",
    "    \n",
    "    # If the request doesn't fail, continue...\n",
    "\n",
    "    # Parse the response HTML using BeautifulSoup\n",
    "    soup = bs.BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    "    # Extract all tables from the response\n",
    "    html_tables = soup.find_all('table')\n",
    "    \n",
    "    # Check that the table we're looking for exists\n",
    "    # If it doesn't, exit\n",
    "    if len(html_tables)<3:\n",
    "        print(\"table too short\")\n",
    "        return\n",
    "    \n",
    "    # Parse the Filings table\n",
    "    filings_table = pd.read_html(str(html_tables[2]), header=0)[0]\n",
    "    filings_table['Filings'] = [str(x) for x in filings_table['Filings']]\n",
    "\n",
    "    # Get only 10-Q document filings\n",
    "    filings_table = filings_table[filings_table['Filings'] == '10-Q']\n",
    "\n",
    "    # If filings table doesn't have any\n",
    "    # 10-Ks or 10-K405s, exit\n",
    "    if len(filings_table) == 0:\n",
    "        return\n",
    "    \n",
    "    # Get accession number for each 10-K and 10-K405 filing\n",
    "    filings_table['Acc_No'] = [x.replace('\\xa0',' ')\n",
    "                               .split('Acc-no: ')[1]\n",
    "                               .split(' ')[0] for x in filings_table['Description']]\n",
    "\n",
    "    # Iterate through each filing and \n",
    "    # scrape the corresponding document...\n",
    "    for index, row in filings_table.iterrows():\n",
    "        if row[\"Filing Date\"] < \"2020-01-01\":\n",
    "            continue\n",
    "        \n",
    "        # Get the accession number for the filing\n",
    "        acc_no = str(row['Acc_No'])\n",
    "        \n",
    "        # Navigate to the page for the filing\n",
    "        docs_page = requests.get(filing_url_base % (cik, acc_no), headers=headers_2)\n",
    "        \n",
    "        # If request fails, log the failure\n",
    "        # and skip to the next filing    \n",
    "        if docs_page.status_code != 200:\n",
    "            text = \"Request failed with error code \" + str(docs_page.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (filing_url_base % (cik, acc_no)) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            continue\n",
    "        \n",
    "        # If request succeeds, keep going...\n",
    "        \n",
    "        # Parse the table of documents for the filing\n",
    "        docs_page_soup = bs.BeautifulSoup(docs_page.text, 'lxml')\n",
    "        docs_html_tables = docs_page_soup.find_all('table')\n",
    "        if len(docs_html_tables)==0:\n",
    "            continue\n",
    "        docs_table = pd.read_html(str(docs_html_tables[0]), header=0)[0]\n",
    "        docs_table['Type'] = [str(x) for x in docs_table['Type']]\n",
    "        \n",
    "        # Get the 10-K and 10-K405 entries for the filing\n",
    "        docs_table = docs_table[docs_table['Type'] == '10-Q']\n",
    "        \n",
    "        # If there aren't any 10-K or 10-K405 entries,\n",
    "        # skip to the next filing\n",
    "        if len(docs_table)==0:\n",
    "            continue\n",
    "        # If there are 10-K or 10-K405 entries,\n",
    "        # grab the first document\n",
    "        elif len(docs_table)>0:\n",
    "            docs_table = docs_table.iloc[0]\n",
    "        \n",
    "        docname = docs_table['Document']\n",
    "        \n",
    "        # If that first entry is unavailable,\n",
    "        # log the failure and exit\n",
    "        if str(docname) == 'nan':\n",
    "            text = 'File with CIK: %s and Acc_No: %s is unavailable' % (cik, acc_no) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            continue       \n",
    "        \n",
    "        # If it is available, continue...\n",
    "        docname = docname.split(' ')[0]\n",
    "        # Request the file\n",
    "        \n",
    "        file = requests.get(doc_url_base % (cik, acc_no.replace('-', ''), docname), headers=headers)\n",
    "        \n",
    "        # If the request fails, log the failure and exit\n",
    "        if file.status_code != 200:\n",
    "            text = \"Request failed with error code \" + str(file.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (doc_url_base % (cik, acc_no.replace('-', ''), docname)) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            continue\n",
    "        \n",
    "        # If it succeeds, keep going...\n",
    "        \n",
    "        # Save the file in appropriate format\n",
    "        \"\"\"\n",
    "        date = str(row['Filing Date'])\n",
    "        filename = cik + '_' + date + '.txt'\n",
    "        html_file = open(filename, 'a')\n",
    "        html_file.write(file.text)\n",
    "        html_file.close()\n",
    "        \"\"\"\n",
    "        if '.txt' in docname:\n",
    "            # Save text as TXT\n",
    "            date = str(row['Filing Date'])\n",
    "            filename = cik + '_' + date + '.txt'\n",
    "            file_path = os.path.join(\"data\", \"10q\", cik, filename)\n",
    "            html_file = open(file_path, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "        else:\n",
    "            # Save text as HTML\n",
    "            date = str(row['Filing Date'])\n",
    "            filename = cik + '_' + date + '.html'\n",
    "            file_path = os.path.join(\"data\", \"10q\", cik, filename)\n",
    "            html_file = open(file_path, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "        \n",
    "    # Move back to the main 10-Q directory\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_10k = './data/10k'\n",
    "dir_10q = './data/10q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0001018724</td>\n",
       "      <td>AMZN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0000764478</td>\n",
       "      <td>BBY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0001075531</td>\n",
       "      <td>BKNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>0000063908</td>\n",
       "      <td>MCD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0001065088</td>\n",
       "      <td>EBAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            cik ticker\n",
       "39   0001018724   AMZN\n",
       "64   0000764478    BBY\n",
       "71   0001075531   BKNG\n",
       "304  0000063908    MCD\n",
       "156  0001065088   EBAY"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_cik_sample[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_esg_score = pd.read_excel(\"data/esg_score.xlsx\", sheet_name = \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>socialScore</th>\n",
       "      <th>governanceScore</th>\n",
       "      <th>environmentScore</th>\n",
       "      <th>totalEsg</th>\n",
       "      <th>percentile</th>\n",
       "      <th>marketCap</th>\n",
       "      <th>dividendRate</th>\n",
       "      <th>fiftyDayAverage</th>\n",
       "      <th>shortRatio</th>\n",
       "      <th>revenueGrowth</th>\n",
       "      <th>debtToEquity</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>C</td>\n",
       "      <td>10.83</td>\n",
       "      <td>12.64</td>\n",
       "      <td>1.75</td>\n",
       "      <td>25.21</td>\n",
       "      <td>41.69</td>\n",
       "      <td>1.383683e+11</td>\n",
       "      <td>2.04</td>\n",
       "      <td>70.96743</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0.583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>Banks—Diversified</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Company  socialScore  governanceScore  environmentScore  totalEsg  \\\n",
       "137       C        10.83            12.64              1.75     25.21   \n",
       "\n",
       "     percentile     marketCap  dividendRate  fiftyDayAverage  shortRatio  \\\n",
       "137       41.69  1.383683e+11          2.04         70.96743        2.35   \n",
       "\n",
       "     revenueGrowth  debtToEquity              sector           industry  \n",
       "137          0.583           NaN  Financial Services  Banks—Diversified  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_esg_score[df_esg_score[\"Company\"] == \"C\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector = \"Financial Services\"\n",
    "tickers = df_esg_score[df_esg_score[\"sector\"] == sector][\"Company\"].values\n",
    "ciks = []\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        # for a given ticker, find its cik number through th ticker library\n",
    "        ciks.append(ticker_cik_df[ticker_cik_df.ticker == ticker][\"cik\"].values[0])\n",
    "    except:\n",
    "        # if could not find cik, give it a empty cik\n",
    "        ciks.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\"WRB\"]\n",
    "ciks = ['0000011544']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0000011544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run the function to scrape 10-Ks\n",
    "\n",
    "# Define parameters\n",
    "browse_url_base_10k = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=%s&type=10-K'\n",
    "filing_url_base_10k = 'http://www.sec.gov/Archives/edgar/data/%s/%s-index.html'\n",
    "doc_url_base_10k = 'http://www.sec.gov/Archives/edgar/data/%s/%s/%s'\n",
    "\n",
    "# Initialize log file\n",
    "# (log file name = the time we initiate scraping session)\n",
    "t = strftime(\"%Y_%m_%d_%H_%M_%S\", gmtime())\n",
    "log_file_name = t + \".txt\"\n",
    "log_file_path = os.path.join(\"log\", log_file_name)\n",
    "\n",
    "with open(log_file_path, 'a') as log_file:\n",
    "    log_file.close()\n",
    "\n",
    "# Iterate over CIKs and scrape 10-Ks\n",
    "for cik in tqdm(ciks):\n",
    "    time.sleep(5)\n",
    "    Scrape10K(browse_url_base=browse_url_base_10k, \n",
    "          filing_url_base=filing_url_base_10k, \n",
    "          doc_url_base=doc_url_base_10k, \n",
    "          cik=cik,\n",
    "          log_file_name=log_file_path)\n",
    "    \n",
    "\n",
    "#return to the main menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0000011544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:17<00:00, 17.27s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run the function to scrape 10-Qs\n",
    "\n",
    "# Define parameters\n",
    "browse_url_base_10q = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=%s&type=10-Q&count=1000'\n",
    "filing_url_base_10q = 'http://www.sec.gov/Archives/edgar/data/%s/%s-index.html'\n",
    "doc_url_base_10q = 'http://www.sec.gov/Archives/edgar/data/%s/%s/%s'\n",
    "\n",
    "# Initialize log file\n",
    "# (log file name = the time we initiate scraping session)\n",
    "t = strftime(\"%Y_%m_%d_%H_%M_%S\", gmtime())\n",
    "log_file_name = t + \".txt\"\n",
    "log_file_path = os.path.join(\"log\", log_file_name)\n",
    "\n",
    "with open(log_file_path, 'a') as log_file:\n",
    "    log_file.close()\n",
    "\n",
    "# Iterate over CIKs and scrape 10-Qs\n",
    "for cik in tqdm(ciks):\n",
    "    time.sleep(5)\n",
    "    Scrape10Q(browse_url_base = browse_url_base_10q, \n",
    "          filing_url_base = filing_url_base_10q, \n",
    "          doc_url_base = doc_url_base_10q, \n",
    "          cik = cik,\n",
    "          log_file_name = log_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert html files to pure txt: preparing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveNumericalTables(soup):\n",
    "    \n",
    "    '''\n",
    "    Removes tables with >15% numerical characters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    soup : BeautifulSoup object\n",
    "        Parsed result from BeautifulSoup.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    soup : BeautifulSoup object\n",
    "        Parsed result from BeautifulSoup\n",
    "        with numerical tables removed.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Determines percentage of numerical characters\n",
    "    # in a table\n",
    "    def GetDigitPercentage(tablestring):\n",
    "        if len(tablestring) > 0.0:\n",
    "            numbers = sum([char.isdigit() for char in tablestring])\n",
    "            length = len(tablestring)\n",
    "            return numbers/length\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    # Evaluates numerical character % for each table\n",
    "    # and removes the table if the percentage is > 15%\n",
    "    [x.extract() for x in soup.find_all('table') if GetDigitPercentage(x.get_text())>0.15]\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveTags(soup):\n",
    "    \n",
    "    '''\n",
    "    Drops HTML tags, newlines and unicode text from\n",
    "    filing text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    soup : BeautifulSoup object\n",
    "        Parsed result from BeautifulSoup.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    text : str\n",
    "        Filing text.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Remove HTML tags with get_text\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Remove newline characters\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    '''\n",
    "    Adding a replace to convert all excecute space to one space \n",
    "    '''\n",
    "    text = re.sub(r'\\s', ' ', text)\n",
    "    \n",
    "    # Replace unicode characters with their\n",
    "    # \"normal\" representations\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertHTML(cik, dirpath):\n",
    "    \n",
    "    '''\n",
    "    Removes numerical tables, HTML tags,\n",
    "    newlines, unicode text, and XBRL tables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cik : str\n",
    "        Central Index Key used to scrape files.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    dir_path = os.path.join(dirpath, cik)\n",
    "    \n",
    "    # Look for files scraped for that CIK\n",
    "    try: \n",
    "        os.path.exists(dir_path)\n",
    "    # ...if we didn't scrape any files for that CIK, exit\n",
    "    except FileNotFoundError:\n",
    "        print(\"Could not find directory for CIK\", cik)\n",
    "        return\n",
    "    \"\"\" avoid print\"\"\"    \n",
    "    # print(\"Parsing CIK %s...\" % cik)\n",
    "    \n",
    "    parsed = False # flag to tell if we've parsed anything\n",
    "    \n",
    "    # Try to make a new directory within the CIK directory\n",
    "    # to store the text representations of the filings\n",
    "    raw_txt_dir = os.path.join(dir_path, \"rawtext\")\n",
    "    try:\n",
    "        os.mkdir(raw_txt_dir)\n",
    "    # If it already exists, continue\n",
    "    # We can't exit at this point because we might be\n",
    "    # partially through parsing text files, so we need to continue\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    # Get list of scraped files\n",
    "    # excluding hidden files and directories\n",
    "    file_list = [fname for fname in os.listdir(dir_path) if not (fname.startswith('.') | os.path.isdir(os.path.join(dir_path, fname)))]\n",
    "    \n",
    "    # Iterate over scraped files and clean\n",
    "    for filename in file_list:\n",
    "            \n",
    "        # Check if file has already been cleaned\n",
    "        new_filename = filename.replace('.html', '.txt')\n",
    "        text_file_list = os.listdir(raw_txt_dir)\n",
    "        if new_filename in text_file_list:\n",
    "            continue\n",
    "        \n",
    "        # If it hasn't been cleaned already, keep going...\n",
    "        \n",
    "        # Clean file\n",
    "        with open(os.path.join(dir_path, filename), 'r', encoding='utf-8') as file:\n",
    "            parsed = True\n",
    "            soup = bs.BeautifulSoup(file.read(), \"lxml\")\n",
    "            soup = RemoveNumericalTables(soup)\n",
    "            \n",
    "            #add my delete reference function here\n",
    "            #soup = delete_reference(soup)\n",
    "            \n",
    "            text = RemoveTags(soup)\n",
    "            with open(os.path.join(raw_txt_dir, new_filename), 'w',encoding='utf-8') as newfile:\n",
    "                newfile.write(text)\n",
    "    \n",
    "    # If all files in the CIK directory have been parsed\n",
    "    # then log that\n",
    "    if parsed==False:\n",
    "        print(\"Already parsed CIK\", cik)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In the documents, there are some parts not in the Management Discussion but contain reference to this part,like:\n",
    "'For a discussion of our contractual obligations, see “Item 7. Management’s Discussion and Analysis of Financial Condition and Results of Operations,”'' \n",
    "\n",
    "That is why I need a function to delete all the reference parts of MD&A.\n",
    "\n",
    "'''\n",
    "\n",
    "#compose a function to delete the references\n",
    "def delete_reference(soup):\n",
    "    \n",
    "    #define string reference to the management's discussion parts\n",
    "    management_str = \"Financial Condition and Results of Operations\"\n",
    "                    \n",
    "    #define string reference to the financial statement parts, which is what we need for parsing\n",
    "    financial_str = \"and Supplementary Data\"\n",
    "    \n",
    "    #given the none reference length for comparision\n",
    "    length_1 = len('Management’s Discussion and Analysis of Financial Condition and Results of Operations')\n",
    "    \n",
    "    #given the none reference length for comparision\n",
    "    length_2 = len('Financial Statements and Supplementary Data')\n",
    "    \n",
    "    #find all the parts contains this reference\n",
    "    for part in soup.find_all(text = re.compile(management_str)):\n",
    "        \n",
    "        #if the lenth is much greater than the original string length, then it's a reference, and we should delete it\n",
    "        if len(part) > length_1:\n",
    "            \n",
    "            #we should extract these text\n",
    "            part.extract()\n",
    "            \n",
    "    #the same should be operated to the financial statements\n",
    "    for part in soup.find_all(text = re.compile(financial_str)):\n",
    "        if len(part) > length_2:\n",
    "            part.extract()\n",
    "    \n",
    "    #return soup for futher parsing\n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert html to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:17<00:00, 17.87s/it]\n"
     ]
    }
   ],
   "source": [
    "# For 10-Ks...\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Iterate over CIKs and clean HTML filings\n",
    "for cik in tqdm(ciks):\n",
    "    ConvertHTML(cik, dir_10k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:17<00:00, 17.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# For 10-Qs...\n",
    "\n",
    "# Iterate over CIKs and clean HTML filings\n",
    "for cik in tqdm(ciks):\n",
    "    ConvertHTML(cik, dir_10q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b893d7568335c74a8df3e912797c1bbf5cf11a7925c935f70126c10891afeb03"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
