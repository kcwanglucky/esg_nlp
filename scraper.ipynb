{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from time import gmtime, strftime\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "# Importing libraries you need to install\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import requests\n",
    "import bs4 as bs\n",
    "from lxml import html\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import time as t_time\n",
    "# from ratelimit import limits\n",
    "from secedgar.filings import Filing, FilingType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read and have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luckywang/Documents/Document/Course Material/Fall 2021/esg_nlp/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data\"\n",
    "\n",
    "# read the ticker library of all the tikers into ticker_library\n",
    "ticker_library = pd.read_csv(os.path.join(data_dir, \"tickers.csv\"))\n",
    "\n",
    "# read the sp500 components into ticker_selected, 'name' is the company name and ticker is company's ticker\n",
    "ticker_selected = pd.read_csv(os.path.join(data_dir, \"sp500_component_stocks.csv\"), header = None)\n",
    "ticker_selected.columns = [\"name\", \"ticker\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a cik&ticker dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001090872</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000006201</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001158449</td>\n",
       "      <td>AAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000320193</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001551152</td>\n",
       "      <td>ABBV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cik ticker\n",
       "0  0001090872      A\n",
       "1  0000006201    AAL\n",
       "2  0001158449    AAP\n",
       "3  0000320193   AAPL\n",
       "4  0001551152   ABBV"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a ticker_cik_df dataframe to store ticker and its cik number\n",
    "ticker_cik_df = pd.DataFrame()\n",
    "\n",
    "# store all the tickers in a ticker_list\n",
    "ticker_list = ticker_selected.ticker\n",
    "\n",
    "# build a list cik_list for cik\n",
    "cik_list = []\n",
    "\n",
    "for ticker in ticker_list:    \n",
    "    try:\n",
    "        # for a given ticker, find its cik number through th ticker library\n",
    "        cik_list.append(list(ticker_library[ticker_library.ticker == ticker].secfilings)[0][-10:])\n",
    "        \n",
    "    except:\n",
    "        # if could not find cik, give it a empty cik\n",
    "        cik_list.append('')\n",
    "\n",
    "# write cik_list and ticker_list to the dataframe ticker_cik_df\n",
    "ticker_cik_df['cik'] = cik_list\n",
    "ticker_cik_df['ticker'] = ticker_list\n",
    "\n",
    "# delete the tickers with empty cik number\n",
    "ticker_cik_df = ticker_cik_df[ticker_cik_df['cik'] != '']\n",
    "\n",
    "# display a sample of ticker_cik_df\n",
    "ticker_cik_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "listtickers = ['AMZN','BBY','BKNG','MCD','EBAY','F','HD','TGT','WHR','JPM','SIVB','CFG','C','ALL','IVZ','ETFC','MET','PFG','CBOE',\n",
    "              'CTL','IPG','VIAC','NFLX','CHTR','FB','TWTR','NWSA','FOXA','AMD','INTC','AAPL','LRCX','MSFT','NLOK','CTSH','ADS',\n",
    "              'WU','PAYC','ABT','CVS','PFE','JNJ','BIIB','INCY','HSIC','WAT','ALGN','EW']\n",
    "\n",
    "ticker_cik_sample = pd.DataFrame()\n",
    "\n",
    "for ticker in listtickers:\n",
    "    ticker_cik_sample = ticker_cik_sample.append(ticker_cik_df[ticker_cik_df['ticker'] == ticker])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0001018724</td>\n",
       "      <td>AMZN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0000764478</td>\n",
       "      <td>BBY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0001075531</td>\n",
       "      <td>BKNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>0000063908</td>\n",
       "      <td>MCD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0001065088</td>\n",
       "      <td>EBAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0000037996</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0000354950</td>\n",
       "      <td>HD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>0000027419</td>\n",
       "      <td>TGT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0000106640</td>\n",
       "      <td>WHR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>0000019617</td>\n",
       "      <td>JPM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0000719739</td>\n",
       "      <td>SIVB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0000759944</td>\n",
       "      <td>CFG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0000831001</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0000899051</td>\n",
       "      <td>ALL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>0000914208</td>\n",
       "      <td>IVZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0001015780</td>\n",
       "      <td>ETFC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>0001099219</td>\n",
       "      <td>MET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>0001126328</td>\n",
       "      <td>PFG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0001374310</td>\n",
       "      <td>CBOE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0000018926</td>\n",
       "      <td>CTL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0000051644</td>\n",
       "      <td>IPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>0001065280</td>\n",
       "      <td>NFLX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0001091667</td>\n",
       "      <td>CHTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0001326801</td>\n",
       "      <td>FB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>0001418091</td>\n",
       "      <td>TWTR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0001564708</td>\n",
       "      <td>NWSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0001754301</td>\n",
       "      <td>FOXA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0000002488</td>\n",
       "      <td>AMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0000050863</td>\n",
       "      <td>INTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000320193</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0000707549</td>\n",
       "      <td>LRCX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>0000789019</td>\n",
       "      <td>MSFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0001058290</td>\n",
       "      <td>CTSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0001101215</td>\n",
       "      <td>ADS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0001365135</td>\n",
       "      <td>WU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0000001800</td>\n",
       "      <td>ABT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0000064803</td>\n",
       "      <td>CVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>0000078003</td>\n",
       "      <td>PFE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>0000200406</td>\n",
       "      <td>JNJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0000875045</td>\n",
       "      <td>BIIB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0000879169</td>\n",
       "      <td>INCY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0001000228</td>\n",
       "      <td>HSIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0001000697</td>\n",
       "      <td>WAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0001097149</td>\n",
       "      <td>ALGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0001099800</td>\n",
       "      <td>EW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            cik ticker\n",
       "39   0001018724   AMZN\n",
       "64   0000764478    BBY\n",
       "71   0001075531   BKNG\n",
       "304  0000063908    MCD\n",
       "156  0001065088   EBAY\n",
       "178  0000037996      F\n",
       "222  0000354950     HD\n",
       "435  0000027419    TGT\n",
       "485  0000106640    WHR\n",
       "266  0000019617    JPM\n",
       "410  0000719739   SIVB\n",
       "95   0000759944    CFG\n",
       "80   0000831001      C\n",
       "28   0000899051    ALL\n",
       "258  0000914208    IVZ\n",
       "169  0001015780   ETFC\n",
       "310  0001099219    MET\n",
       "366  0001126328    PFG\n",
       "85   0001374310   CBOE\n",
       "124  0000018926    CTL\n",
       "250  0000051644    IPG\n",
       "338  0001065280   NFLX\n",
       "98   0001091667   CHTR\n",
       "181  0001326801     FB\n",
       "449  0001418091   TWTR\n",
       "353  0001564708   NWSA\n",
       "196  0001754301   FOXA\n",
       "33   0000002488    AMD\n",
       "247  0000050863   INTC\n",
       "3    0000320193   AAPL\n",
       "294  0000707549   LRCX\n",
       "326  0000789019   MSFT\n",
       "125  0001058290   CTSH\n",
       "13   0001101215    ADS\n",
       "491  0001365135     WU\n",
       "7    0000001800    ABT\n",
       "128  0000064803    CVS\n",
       "365  0000078003    PFE\n",
       "264  0000200406    JNJ\n",
       "69   0000875045   BIIB\n",
       "245  0000879169   INCY\n",
       "236  0001000228   HSIC\n",
       "478  0001000697    WAT\n",
       "26   0001097149   ALGN\n",
       "173  0001099800     EW"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#F or demostration purposes we will only scrape two companies\n",
    "# ticker_cik_sample = ticker_cik_df.sample(n=2, replace=False, random_state=0)\n",
    "ticker_cik_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import function to scrape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WriteLogFile(log_file_name, text):\n",
    "    '''\n",
    "    Helper function.\n",
    "    Writes a log file with all notes and\n",
    "    error messages from a scraping \"session\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    log_file_name : str\n",
    "        Name of the log file (should be a .txt file).\n",
    "    text : str\n",
    "        Text to write to the log file.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    with open(log_file_name, \"a+\") as log_file:\n",
    "        log_file.write(text)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Safari/605.1.15 Version/13.0.4\",\n",
    "          \"referer\": \"http://localhost:8888/\",\n",
    "          \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_2 = {\"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\",\n",
    "          \"referer\": \"http://localhost:8888/\",\n",
    "          \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape10K(browse_url_base, filing_url_base, doc_url_base, cik, log_file_name):\n",
    "    \n",
    "    '''\n",
    "    Scrapes all 10-Ks and 10-K405s for a particular \n",
    "    CIK from EDGAR.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    browse_url_base : str\n",
    "        Base URL for browsing EDGAR.\n",
    "    filing_url_base : str\n",
    "        Base URL for filings listings on EDGAR.\n",
    "    doc_url_base : str\n",
    "        Base URL for one filing's document tables\n",
    "        page on EDGAR.\n",
    "    cik : str\n",
    "        Central Index Key.\n",
    "    log_file_name : str\n",
    "        Name of the log file (should be a .txt file).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    data_dir = os.path.join(\"data\", \"10k\", cik)\n",
    "    # Check if we've already scraped this CIK\n",
    "    try:\n",
    "        os.mkdir(data_dir)\n",
    "    except OSError:\n",
    "        print(\"Already scraped CIK\", cik)\n",
    "        return\n",
    "    \n",
    "    print('Scraping CIK', cik)\n",
    "    \n",
    "    # Request list of 10-K filings\n",
    "    res = requests.get(browse_url_base % cik, headers=headers)\n",
    "    \n",
    "    # If the request failed, log the failure and exit\n",
    "    if res.status_code != 200:\n",
    "        os.rmdir(data_dir) # remove empty dir\n",
    "        text = \"Request failed with error code \" + str(res.status_code) + \\\n",
    "               \"\\nFailed URL: \" + (browse_url_base % cik) + '\\n'\n",
    "        print(\"main\", text)\n",
    "        WriteLogFile(log_file_name, text)\n",
    "        return\n",
    "    else:\n",
    "        WriteLogFile(log_file_name, \"Success URL: {}\\n\".format(browse_url_base))\n",
    "\n",
    "    # If the request doesn't fail, continue...\n",
    "    \n",
    "    print(res)\n",
    "    \n",
    "    # Parse the response HTML using BeautifulSoup\n",
    "    soup = bs.BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    "    # Extract all tables from the response\n",
    "    html_tables = soup.find_all('table')\n",
    "    \n",
    "    # Check that the table we're looking for exists\n",
    "    # If it doesn't, exit\n",
    "    if len(html_tables) < 3:\n",
    "        return\n",
    "    \n",
    "    # Parse the Filings table\n",
    "    filings_table = pd.read_html(str(html_tables[2]), header=0)[0]\n",
    "    filings_table['Filings'] = [str(x) for x in filings_table['Filings']]\n",
    "\n",
    "    # Get only 10-K and 10-K405 document filings\n",
    "    filings_table = filings_table[(filings_table['Filings'] == '10-K') | (filings_table['Filings'] == '10-K405')]\n",
    "\n",
    "    # If filings table doesn't have any\n",
    "    # 10-Ks or 10-K405s, exit\n",
    "    if len(filings_table) == 0:\n",
    "        return\n",
    "    \n",
    "    # Get accession number for each 10-K and 10-K405 filing\n",
    "    filings_table['Acc_No'] = [x.replace('\\xa0',' ')\n",
    "                               .split('Acc-no: ')[1]\n",
    "                               .split(' ')[0] for x in filings_table['Description']]\n",
    "\n",
    "    # Iterate through each filing and \n",
    "    # scrape the corresponding document...\n",
    "    for index, row in filings_table.iterrows():\n",
    "        \n",
    "        # Get the accession number for the filing\n",
    "        acc_no = str(row['Acc_No'])\n",
    "        \n",
    "        # Navigate to the page for the filing\n",
    "        docs_page = requests.get(filing_url_base % (cik, acc_no), headers=headers_2)\n",
    "        \n",
    "        # If request fails, log the failure\n",
    "        # and skip to the next filing\n",
    "        if docs_page.status_code != 200:\n",
    "            text = \"Request failed with error code \" + str(docs_page.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (filing_url_base % (cik, acc_no)) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            continue\n",
    "        else:\n",
    "            WriteLogFile(log_file_name, \"Success URL: {}\\n\".format(filing_url_base % (cik, acc_no)))\n",
    "\n",
    "        # If request succeeds, keep going...\n",
    "        \n",
    "        # Parse the table of documents for the filing\n",
    "        docs_page_soup = bs.BeautifulSoup(docs_page.text, 'lxml')\n",
    "        docs_html_tables = docs_page_soup.find_all('table')\n",
    "        if len(docs_html_tables)==0:\n",
    "            continue\n",
    "        docs_table = pd.read_html(str(docs_html_tables[0]), header=0)[0]\n",
    "        docs_table['Type'] = [str(x) for x in docs_table['Type']]\n",
    "        \n",
    "        # Get the 10-K and 10-K405 entries for the filing\n",
    "        docs_table = docs_table[(docs_table['Type'] == '10-K') | (docs_table['Type'] == '10-K405')]\n",
    "        \n",
    "        # If there aren't any 10-K or 10-K405 entries,\n",
    "        # skip to the next filing\n",
    "        if len(docs_table)==0:\n",
    "            continue\n",
    "        # If there are 10-K or 10-K405 entries,\n",
    "        # grab the first document\n",
    "        elif len(docs_table)>0:\n",
    "            docs_table = docs_table.iloc[0]\n",
    "        \n",
    "        docname = docs_table['Document']\n",
    "        \n",
    "        # If that first entry is unavailable,\n",
    "        # log the failure and exit\n",
    "        if str(docname) == 'nan':\n",
    "            text = 'File with CIK: %s and Acc_No: %s is unavailable' % (cik, acc_no) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            continue       \n",
    "        \n",
    "        # If it is available, continue...\n",
    "        \n",
    "        # Request the file\n",
    "        file = requests.get(doc_url_base % (cik, acc_no.replace('-', ''), docname), headers=headers)\n",
    "        \n",
    "        # If the request fails, log the failure and exit\n",
    "        if file.status_code != 200:\n",
    "            text = \"Request failed with error code \" + str(file.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (doc_url_base % (cik, acc_no.replace('-', ''), docname)) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            continue\n",
    "        \n",
    "        # If it succeeds, keep going...\n",
    "        \n",
    "        # Save the file in appropriate format\n",
    "        \"\"\"\n",
    "        date = str(row['Filing Date'])\n",
    "        filename = cik + '_' + date + '.txt'\n",
    "        html_file = open(filename, 'a')\n",
    "        html_file.write(file.text)\n",
    "        html_file.close()\n",
    "        \"\"\"\n",
    "        if '.txt' in docname:\n",
    "            # Save text as TXT\n",
    "            date = str(row['Filing Date'])\n",
    "            filename = cik + '_' + date + '.txt'\n",
    "            file_path = os.path.join(\"data\", \"10k\", cik, filename)\n",
    "            html_file = open(file_path, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "        else:\n",
    "            # Save text as HTML\n",
    "            date = str(row['Filing Date'])\n",
    "            filename = cik + '_' + date + '.html'\n",
    "            file_path = os.path.join(\"data\", \"10k\", cik, filename)\n",
    "            html_file = open(file_path, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "        \n",
    "#     # Move back to the main 10-K directory\n",
    "#     os.chdir('..')\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape10Q(browse_url_base, filing_url_base, doc_url_base, cik, log_file_name):\n",
    "    \n",
    "    '''\n",
    "    Scrapes all 10-Qs for a particular CIK from EDGAR.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    browse_url_base : str\n",
    "        Base URL for browsing EDGAR.\n",
    "    filing_url_base : str\n",
    "        Base URL for filings listings on EDGAR.\n",
    "    doc_url_base : str\n",
    "        Base URL for one filing's document tables\n",
    "        page on EDGAR.\n",
    "    cik : str\n",
    "        Central Index Key.\n",
    "    log_file_name : str\n",
    "        Name of the log file (should be a .txt file).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Check if we've already scraped this CIK\n",
    "    data_dir = os.path.join(\"data\", \"10q\", cik)\n",
    "    try:\n",
    "        os.mkdir(data_dir)\n",
    "    except OSError:\n",
    "        print(\"Already scraped CIK\", cik)\n",
    "        return\n",
    "    \n",
    "    # If we haven't, go into the directory for that CIK\n",
    "    \n",
    "    #I avoid print here\n",
    "    \"\"\"\n",
    "    print('Scraping CIK', cik)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Request list of 10-Q filings\n",
    "    res = requests.get(browse_url_base % cik, headers=headers)\n",
    "    \n",
    "    # If the request failed, log the failure and exit\n",
    "    if res.status_code != 200:\n",
    "        os.rmdir(data_dir) # remove empty dir\n",
    "        text = \"Request failed with error code \" + str(res.status_code) + \\\n",
    "               \"\\nFailed URL: \" + (browse_url_base % cik) + '\\n'\n",
    "        print(\"main\", text)\n",
    "        WriteLogFile(log_file_name, text)\n",
    "        return\n",
    "    \n",
    "    # If the request doesn't fail, continue...\n",
    "\n",
    "    # Parse the response HTML using BeautifulSoup\n",
    "    soup = bs.BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    "    # Extract all tables from the response\n",
    "    html_tables = soup.find_all('table')\n",
    "    \n",
    "    # Check that the table we're looking for exists\n",
    "    # If it doesn't, exit\n",
    "    if len(html_tables)<3:\n",
    "        print(\"table too short\")\n",
    "        return\n",
    "    \n",
    "    # Parse the Filings table\n",
    "    filings_table = pd.read_html(str(html_tables[2]), header=0)[0]\n",
    "    filings_table['Filings'] = [str(x) for x in filings_table['Filings']]\n",
    "\n",
    "    # Get only 10-Q document filings\n",
    "    filings_table = filings_table[filings_table['Filings'] == '10-Q']\n",
    "\n",
    "    # If filings table doesn't have any\n",
    "    # 10-Ks or 10-K405s, exit\n",
    "    if len(filings_table)==0:\n",
    "        return\n",
    "    \n",
    "    # Get accession number for each 10-K and 10-K405 filing\n",
    "    filings_table['Acc_No'] = [x.replace('\\xa0',' ')\n",
    "                               .split('Acc-no: ')[1]\n",
    "                               .split(' ')[0] for x in filings_table['Description']]\n",
    "\n",
    "    # Iterate through each filing and \n",
    "    # scrape the corresponding document...\n",
    "    for index, row in filings_table.iterrows():\n",
    "        \n",
    "        # Get the accession number for the filing\n",
    "        acc_no = str(row['Acc_No'])\n",
    "        \n",
    "        # Navigate to the page for the filing\n",
    "        docs_page = requests.get(filing_url_base % (cik, acc_no), headers=headers_2)\n",
    "        \n",
    "        # If request fails, log the failure\n",
    "        # and skip to the next filing    \n",
    "        if docs_page.status_code != 200:\n",
    "            text = \"Request failed with error code \" + str(docs_page.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (filing_url_base % (cik, acc_no)) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            continue\n",
    "            \n",
    "        # If request succeeds, keep going...\n",
    "        \n",
    "        # Parse the table of documents for the filing\n",
    "        docs_page_soup = bs.BeautifulSoup(docs_page.text, 'lxml')\n",
    "        docs_html_tables = docs_page_soup.find_all('table')\n",
    "        if len(docs_html_tables)==0:\n",
    "            continue\n",
    "        docs_table = pd.read_html(str(docs_html_tables[0]), header=0)[0]\n",
    "        docs_table['Type'] = [str(x) for x in docs_table['Type']]\n",
    "        \n",
    "        # Get the 10-K and 10-K405 entries for the filing\n",
    "        docs_table = docs_table[docs_table['Type'] == '10-Q']\n",
    "        \n",
    "        # If there aren't any 10-K or 10-K405 entries,\n",
    "        # skip to the next filing\n",
    "        if len(docs_table)==0:\n",
    "            continue\n",
    "        # If there are 10-K or 10-K405 entries,\n",
    "        # grab the first document\n",
    "        elif len(docs_table)>0:\n",
    "            docs_table = docs_table.iloc[0]\n",
    "        \n",
    "        docname = docs_table['Document']\n",
    "        \n",
    "        # If that first entry is unavailable,\n",
    "        # log the failure and exit\n",
    "        if str(docname) == 'nan':\n",
    "            text = 'File with CIK: %s and Acc_No: %s is unavailable' % (cik, acc_no) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            continue       \n",
    "        \n",
    "        # If it is available, continue...\n",
    "        \n",
    "        # Request the file\n",
    "        file = requests.get(doc_url_base % (cik, acc_no.replace('-', ''), docname), headers=headers)\n",
    "        \n",
    "        # If the request fails, log the failure and exit\n",
    "        if file.status_code != 200:\n",
    "            text = \"Request failed with error code \" + str(file.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (doc_url_base % (cik, acc_no.replace('-', ''), docname)) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            continue\n",
    "            \n",
    "        # If it succeeds, keep going...\n",
    "        \n",
    "        # Save the file in appropriate format\n",
    "        \"\"\"\n",
    "        date = str(row['Filing Date'])\n",
    "        filename = cik + '_' + date + '.txt'\n",
    "        html_file = open(filename, 'a')\n",
    "        html_file.write(file.text)\n",
    "        html_file.close()\n",
    "        \"\"\"\n",
    "        if '.txt' in docname:\n",
    "            # Save text as TXT\n",
    "            date = str(row['Filing Date'])\n",
    "            filename = cik + '_' + date + '.txt'\n",
    "            file_path = os.path.join(\"data\", \"10q\", cik, filename)\n",
    "            html_file = open(file_path, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "        else:\n",
    "            # Save text as HTML\n",
    "            date = str(row['Filing Date'])\n",
    "            filename = cik + '_' + date + '.html'\n",
    "            file_path = os.path.join(\"data\", \"10q\", cik, filename)\n",
    "            html_file = open(file_path, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "        \n",
    "    # Move back to the main 10-Q directory\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_10k = './data/10k'\n",
    "dir_10q = './data/10q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0000816284</td>\n",
       "      <td>CELG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>0000049826</td>\n",
       "      <td>ITW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            cik ticker\n",
       "92   0000816284   CELG\n",
       "257  0000049826    ITW"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_cik_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_filings = Filing(cik_lookup='aapl', filing_type=FilingType.FILING_10Q, user_agent=\"kcwanglucky@gmail.com\")\n",
    "my_filings.save('aapl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/luckywang/Documents/Document/Course Material/Fall 2021/esg_nlp\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = requests.get(\"http://www.sec.gov/Archives/edgar/data/0000049826/000095013701000857/c60672e10-k405.htm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▋                                                                                           | 1/35 [00:05<02:50,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already scraped CIK 0000719739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|█████▎                                                                                        | 2/35 [00:10<02:45,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already scraped CIK 0000759944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|████████                                                                                      | 3/35 [00:15<02:40,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already scraped CIK 0000831001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|██████████▋                                                                                   | 4/35 [00:20<02:35,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already scraped CIK 0000899051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█████████████▍                                                                                | 5/35 [00:25<02:30,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already scraped CIK 0000914208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|████████████████                                                                              | 6/35 [00:30<02:25,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already scraped CIK 0001015780\n",
      "Scraping CIK 0001099219\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██████████████████▊                                                                           | 7/35 [01:09<07:32, 16.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0001126328\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|█████████████████████▍                                                                        | 8/35 [01:32<08:19, 18.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0001374310\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|████████████████████████▏                                                                     | 9/35 [01:57<08:51, 20.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0000018926\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██████████████████████████▌                                                                  | 10/35 [02:29<09:58, 23.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0000051644\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|█████████████████████████████▏                                                               | 11/35 [02:55<09:55, 24.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0001065280\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███████████████████████████████▉                                                             | 12/35 [03:13<08:38, 22.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0001091667\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|██████████████████████████████████▌                                                          | 13/35 [03:33<08:00, 21.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0001326801\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████████▏                                                       | 14/35 [03:45<06:34, 18.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0001418091\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|███████████████████████████████████████▊                                                     | 15/35 [03:54<05:19, 15.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0001564708\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|██████████████████████████████████████████▌                                                  | 16/35 [04:05<04:31, 14.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0001754301\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|█████████████████████████████████████████████▏                                               | 17/35 [04:12<03:41, 12.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0000002488\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|███████████████████████████████████████████████▊                                             | 18/35 [04:36<04:28, 15.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0000050863\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|██████████████████████████████████████████████████▍                                          | 19/35 [05:07<05:27, 20.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0000320193\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████████████████████████████████████████████████████▏                                       | 20/35 [05:28<05:07, 20.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0000707549\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████████████████████████████████▊                                     | 21/35 [06:09<06:12, 26.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0000789019\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████████████████████████████████████████████████████████▍                                  | 22/35 [06:29<05:20, 24.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0001058290\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|█████████████████████████████████████████████████████████████                                | 23/35 [06:50<04:42, 23.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0001101215\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|███████████████████████████████████████████████████████████████▊                             | 24/35 [07:12<04:13, 23.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0001365135\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|██████████████████████████████████████████████████████████████████▍                          | 25/35 [07:27<03:28, 20.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0000001800\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|█████████████████████████████████████████████████████████████████████                        | 26/35 [07:48<03:06, 20.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0000064803\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████████████████████████████████████████████████████████████████████▋                     | 27/35 [08:10<02:49, 21.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0000078003\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████████████▍                  | 28/35 [08:31<02:28, 21.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0000200406\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|█████████████████████████████████████████████████████████████████████████████                | 29/35 [08:56<02:13, 22.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0000875045\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|███████████████████████████████████████████████████████████████████████████████▋             | 30/35 [09:29<02:06, 25.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0000879169\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|██████████████████████████████████████████████████████████████████████████████████▎          | 31/35 [10:15<02:06, 31.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0001000228\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████████████████████████████████████████████████████████████████████████████████        | 32/35 [11:17<02:01, 40.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0001000697\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|███████████████████████████████████████████████████████████████████████████████████████▋     | 33/35 [12:02<01:23, 41.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0001097149\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|██████████████████████████████████████████████████████████████████████████████████████████▎  | 34/35 [12:21<00:35, 35.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0001099800\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [12:38<00:00, 21.68s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run the function to scrape 10-Ks\n",
    "\n",
    "# Define parameters\n",
    "browse_url_base_10k = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=%s&type=10-K'\n",
    "filing_url_base_10k = 'http://www.sec.gov/Archives/edgar/data/%s/%s-index.html'\n",
    "doc_url_base_10k = 'http://www.sec.gov/Archives/edgar/data/%s/%s/%s'\n",
    "\n",
    "# Initialize log file\n",
    "# (log file name = the time we initiate scraping session)\n",
    "t = strftime(\"%Y_%m_%d_%H_%M_%S\", gmtime())\n",
    "log_file_name = t + \".txt\"\n",
    "log_file_path = os.path.join(\"log\", log_file_name)\n",
    "\n",
    "with open(log_file_path, 'a') as log_file:\n",
    "    log_file.close()\n",
    "\n",
    "# Iterate over CIKs and scrape 10-Ks\n",
    "for cik in tqdm(ticker_cik_sample['cik']):\n",
    "    time.sleep(5)\n",
    "    Scrape10K(browse_url_base=browse_url_base_10k, \n",
    "          filing_url_base=filing_url_base_10k, \n",
    "          doc_url_base=doc_url_base_10k, \n",
    "          cik=cik,\n",
    "          log_file_name=log_file_path)\n",
    "    \n",
    "\n",
    "#return to the main menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██                                                                                            | 1/45 [00:03<02:12,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already scraped CIK 0001018724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|████▏                                                                                         | 2/45 [00:06<02:09,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already scraped CIK 0000764478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|██████████▍                                                                                   | 5/45 [02:26<25:03, 37.58s/it]"
     ]
    }
   ],
   "source": [
    "# Run the function to scrape 10-Qs\n",
    "\n",
    "# Define parameters\n",
    "browse_url_base_10q = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=%s&type=10-Q&count=1000'\n",
    "filing_url_base_10q = 'http://www.sec.gov/Archives/edgar/data/%s/%s-index.html'\n",
    "doc_url_base_10q = 'http://www.sec.gov/Archives/edgar/data/%s/%s/%s'\n",
    "\n",
    "# Initialize log file\n",
    "# (log file name = the time we initiate scraping session)\n",
    "t = strftime(\"%Y_%m_%d_%H_%M_%S\", gmtime())\n",
    "log_file_name = t + \".txt\"\n",
    "log_file_path = os.path.join(\"log\", log_file_name)\n",
    "\n",
    "# Iterate over CIKs and scrape 10-Qs\n",
    "for cik in tqdm(ticker_cik_sample['cik']):\n",
    "    time.sleep(3)\n",
    "    Scrape10Q(browse_url_base = browse_url_base_10q, \n",
    "          filing_url_base = filing_url_base_10q, \n",
    "          doc_url_base = doc_url_base_10q, \n",
    "          cik = cik,\n",
    "          log_file_name = log_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert html files to pure txt: preparing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveNumericalTables(soup):\n",
    "    \n",
    "    '''\n",
    "    Removes tables with >15% numerical characters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    soup : BeautifulSoup object\n",
    "        Parsed result from BeautifulSoup.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    soup : BeautifulSoup object\n",
    "        Parsed result from BeautifulSoup\n",
    "        with numerical tables removed.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Determines percentage of numerical characters\n",
    "    # in a table\n",
    "    def GetDigitPercentage(tablestring):\n",
    "        if len(tablestring)>0.0:\n",
    "            numbers = sum([char.isdigit() for char in tablestring])\n",
    "            length = len(tablestring)\n",
    "            return numbers/length\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    # Evaluates numerical character % for each table\n",
    "    # and removes the table if the percentage is > 15%\n",
    "    [x.extract() for x in soup.find_all('table') if GetDigitPercentage(x.get_text())>0.15]\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveTags(soup):\n",
    "    \n",
    "    '''\n",
    "    Drops HTML tags, newlines and unicode text from\n",
    "    filing text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    soup : BeautifulSoup object\n",
    "        Parsed result from BeautifulSoup.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    text : str\n",
    "        Filing text.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Remove HTML tags with get_text\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Remove newline characters\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    '''\n",
    "    Adding a replace to convert all excecute space to one space \n",
    "    '''\n",
    "    text = re.sub(r'\\s', ' ', text)\n",
    "    \n",
    "    # Replace unicode characters with their\n",
    "    # \"normal\" representations\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertHTML(cik):\n",
    "    \n",
    "    '''\n",
    "    Removes numerical tables, HTML tags,\n",
    "    newlines, unicode text, and XBRL tables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cik : str\n",
    "        Central Index Key used to scrape files.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Look for files scraped for that CIK\n",
    "    try: \n",
    "        os.chdir(cik)\n",
    "    # ...if we didn't scrape any files for that CIK, exit\n",
    "    except FileNotFoundError:\n",
    "        print(\"Could not find directory for CIK\", cik)\n",
    "        return\n",
    "    \"\"\" avoid print\"\"\"    \n",
    "    #print(\"Parsing CIK %s...\" % cik)\n",
    "    \n",
    "    parsed = False # flag to tell if we've parsed anything\n",
    "    \n",
    "    # Try to make a new directory within the CIK directory\n",
    "    # to store the text representations of the filings\n",
    "    try:\n",
    "        os.mkdir('rawtext')\n",
    "    # If it already exists, continue\n",
    "    # We can't exit at this point because we might be\n",
    "    # partially through parsing text files, so we need to continue\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    # Get list of scraped files\n",
    "    # excluding hidden files and directories\n",
    "    file_list = [fname for fname in os.listdir() if not (fname.startswith('.') | os.path.isdir(fname))]\n",
    "    \n",
    "    # Iterate over scraped files and clean\n",
    "    for filename in file_list:\n",
    "            \n",
    "        # Check if file has already been cleaned\n",
    "        new_filename = filename.replace('.html', '.txt')\n",
    "        text_file_list = os.listdir('rawtext')\n",
    "        if new_filename in text_file_list:\n",
    "            continue\n",
    "        \n",
    "        # If it hasn't been cleaned already, keep going...\n",
    "        \n",
    "        # Clean file\n",
    "        with open(filename, 'r',encoding='utf-8') as file:\n",
    "            parsed = True\n",
    "            soup = bs.BeautifulSoup(file.read(), \"lxml\")\n",
    "            soup = RemoveNumericalTables(soup)\n",
    "            \n",
    "            #add my delete reference function here\n",
    "            #soup = delete_reference(soup)\n",
    "            \n",
    "            text = RemoveTags(soup)\n",
    "            with open('rawtext/'+new_filename, 'w',encoding='utf-8') as newfile:\n",
    "                newfile.write(text)\n",
    "    \n",
    "    # If all files in the CIK directory have been parsed\n",
    "    # then log that\n",
    "    if parsed==False:\n",
    "        print(\"Already parsed CIK\", cik)\n",
    "    \n",
    "    os.chdir('..')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In the documents, there are some parts not in the Management Discussion but contain reference to this part,like:\n",
    "'For a discussion of our contractual obligations, see “Item 7. Management’s Discussion and Analysis of Financial Condition and Results of Operations,”'' \n",
    "\n",
    "That is why I need a function to delete all the reference parts of MD&A.\n",
    "\n",
    "'''\n",
    "\n",
    "#compose a function to delete the references\n",
    "def delete_reference(soup):\n",
    "    \n",
    "    #define string reference to the management's discussion parts\n",
    "    management_str = \"Financial Condition and Results of Operations\"\n",
    "                    \n",
    "    #define string reference to the financial statement parts, which is what we need for parsing\n",
    "    financial_str = \"and Supplementary Data\"\n",
    "    \n",
    "    #given the none reference length for comparision\n",
    "    length_1 = len('Management’s Discussion and Analysis of Financial Condition and Results of Operations')\n",
    "    \n",
    "    #given the none reference length for comparision\n",
    "    length_2 = len('Financial Statements and Supplementary Data')\n",
    "    \n",
    "    #find all the parts contains this reference\n",
    "    for part in soup.find_all(text = re.compile(management_str)):\n",
    "        \n",
    "        #if the lenth is much greater than the original string length, then it's a reference, and we should delete it\n",
    "        if len(part) > length_1:\n",
    "            \n",
    "            #we should extract these text\n",
    "            part.extract()\n",
    "            \n",
    "    #the same should be operated to the financial statements\n",
    "    for part in soup.find_all(text = re.compile(financial_str)):\n",
    "        if len(part) > length_2:\n",
    "            part.extract()\n",
    "    \n",
    "    #return soup for futher parsing\n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert html to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:29<00:00, 14.88s/it]\n"
     ]
    }
   ],
   "source": [
    "# For 10-Ks...\n",
    "# -*- coding: utf-8 -*-\n",
    "os.chdir(pathname_10k)\n",
    "\n",
    "# Iterate over CIKs and clean HTML filings\n",
    "for cik in tqdm(ticker_cik_sample['cik']):\n",
    "    ConvertHTML(cik)\n",
    "os.chdir('..')\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:50<00:00, 25.08s/it]\n"
     ]
    }
   ],
   "source": [
    "# For 10-Qs...\n",
    "\n",
    "os.chdir(pathname_10q)\n",
    "\n",
    "# Iterate over CIKs and clean HTML filings\n",
    "for cik in tqdm(ticker_cik_sample['cik']):\n",
    "    ConvertHTML(cik)\n",
    "os.chdir('..')\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/silviaruiz/Documents/Code/Github'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
