{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/luckywang/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import linregress\n",
    "from utils.preprocessing import get_texts\n",
    "from utils.preprocessing import get_texts, stop_words\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix \n",
    "import nltk\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_esg_score = pd.read_excel(\"data/esg_score.xlsx\", sheet_name = \"data\")\n",
    "df_esg_score = df_esg_score.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luckywang/Documents/Document/Course Material/Fall 2021/esg_nlp/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "ticker_library = pd.read_csv(os.path.join(\"data\", \"tickers.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_esg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_comp_training = pd.DataFrame(index = ['score'])\n",
    "# validate_comp_train = pd.DataFrame(index = ['score'])\n",
    "results_training = pd.DataFrame(index=['accuracy', 'precision', 'recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luckywang/Documents/Document/Course Material/Fall 2021/esg_nlp/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "ticker_library = pd.read_csv(os.path.join(\"data\", \"tickers.csv\"))\n",
    "\n",
    "def get_cik(ticker):\n",
    "    \"\"\" Get the cik for the ticker specified by the input argument \n",
    "    Input:\n",
    "        ticker(str): ticker of the company e.g. \"FB\"\n",
    "    \"\"\"\n",
    "    return ticker_library[ticker_library.ticker == ticker].secfilings.values[0][-10:]\n",
    "    \n",
    "def ngrams(s, n):\n",
    "    \"\"\" Get all the n-gram for input texts s\n",
    "    Input:\n",
    "        s (str): A string of texts with each word separated by a whitespace\n",
    "        n (int): n-gram to extract\n",
    "    Return:\n",
    "        [str]: A list of string in the following format ([['a', 'b'], ['b', 'c'], ['c', 'd']])\n",
    "    \"\"\"\n",
    "    \n",
    "    s = s.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(s) - n + 1):\n",
    "        output.append(s[i:i+n])\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_count(doc, df_dict, n_min, n_max):\n",
    "    \"\"\" Count the number of good and bad words occurred in the document\n",
    "    Input:\n",
    "        doc (str): A string with all the words in the documents\n",
    "        df_dict (pd.DataFrame): A DataFrame with word and isGood column, generated by previous section\n",
    "        n_min, n_max (int): specify the ngram range used to generate the dictionary, should be consistent with how df_dict is generated\n",
    "    Return:\n",
    "        (dict): A dictionary with value good_count and bad_count\n",
    "    \"\"\"\n",
    "    # grams = doc.split()\n",
    "\n",
    "    grams = []\n",
    "    for n in range(n_min, n_max + 1):\n",
    "        grams.extend([' '.join(li) for li in ngrams(doc, n)])\n",
    "\n",
    "    good_count = bad_count = 0\n",
    "    s = 0\n",
    "    \n",
    "    for g in grams:\n",
    "        if g in df_dict[\"word\"].values:\n",
    "            val = df_dict[df_dict[\"word\"] == g][\"isGood\"].values\n",
    "            if val > 0:\n",
    "                good_count += 1\n",
    "            elif val <= 0:\n",
    "                bad_count += 1\n",
    "            s += val\n",
    "    print(s)\n",
    "    return {\"good_count\": good_count, \"bad_count\": bad_count, \"score\": s}\n",
    "\n",
    "def normalize(array):\n",
    "    mean = np.mean(array)\n",
    "    std = np.std(array)\n",
    "\n",
    "    return (array - mean) / std\n",
    "\n",
    "    \n",
    "def validation(df_topk, val_tickers, score_scheme=\"binary\", alpha=0.3):\n",
    "    \"\"\" Perform the validation step\n",
    "    The validation rationale: Companies whose score are in upper 50% group are considered \"bad\" companies and the corresponding val_true = 1; 0 otherwise (in lower 50% group, which is considered a good company)\n",
    "    Input:\n",
    "        df_topk (pd.DataFrame): containes the sector specific dict\n",
    "        val_tickers (list): A list of tickers to be validated\n",
    "        score_scheme: score_scheme in (\"binary\", \"ratio\", \"ratio_idf\")\n",
    "    \"\"\"\n",
    "    assert score_scheme in (\"binary\", \"ratio\", \"ratio_idf\", \"ratio_norm\", \"ratio_kernel\")\n",
    "    if score_scheme == \"binary\":\n",
    "        diff = df_topk[\"good_nums\"] - df_topk[\"bad_nums\"]\n",
    "        upper_threshold = np.quantile(diff, 1 - alpha)\n",
    "        lower_threshold = np.quantile(diff, alpha)\n",
    "\n",
    "        df_topk[\"isGood\"] = diff.apply(lambda x: 1 if x > upper_threshold else (\n",
    "        -1 if x < lower_threshold else 0))\n",
    "    elif score_scheme == \"ratio\":\n",
    "        # log(good_count / (bad_count + 1))\n",
    "        # Make sure 80/40 and 40/80 have same magnitude after transformation\n",
    "        df_topk[\"isGood\"] = np.log2((df_topk[\"good_nums\"] + 1) / (df_topk[\"bad_nums\"] + 1))\n",
    "\n",
    "    # elif score_scheme == \"ratio_idf\":\n",
    "    #     # log(good_count / (bad_count + 1))\n",
    "    #     # Make sure 80/40 and 40/80 have same magnitude after transformation\n",
    "    #     total = df_topk[\"good_nums\"].values + df_topk[\"bad_nums\"].values\n",
    "    #     ma = np.max(total)\n",
    "    #     df_topk[\"isGood\"] = ((df_topk[\"good_nums\"] + 1) / (df_topk[\"bad_nums\"] + 1)) * np.log(ma / total)\n",
    "\n",
    "    elif score_scheme == \"ratio_norm\":\n",
    "        # log(good_normalization / bad_normalization)\n",
    "        good_norm = df_topk[\"good_nums\"].values\n",
    "        bad_norm = df_topk[\"bad_nums\"].values\n",
    "        df_topk[\"isGood\"] = np.log2((good_norm + 1)/(bad_norm + 1))\n",
    "    \n",
    "    elif score_scheme == \"ratio_kernel\":\n",
    "        # good_nums / (bad_nums + 1) / total\n",
    "        good_norm = df_topk[\"good_nums\"].values\n",
    "        bad_norm = df_topk[\"bad_nums\"].values\n",
    "        total = df_topk[\"good_nums\"].values + df_topk[\"bad_nums\"].values\n",
    "        ma = np.max(total)\n",
    "        weights = 1 - ((total - ma) ** 2 / (ma ** 2))       \n",
    "        # weight function rationale: Words with high or low occurrence -> not preferrable\n",
    "        # values in between should have higher weights\n",
    "        df_topk[\"isGood\"] = np.log2((good_norm + 1)/(bad_norm + 1) * weights)\n",
    "\n",
    "\n",
    "    print(df_topk)\n",
    "    # 1 if good_nums - bad_nums > threshold; -1 if good_nums - bad_nums < -threshold; 0 otherwise\n",
    "    val_ciks = [get_cik(ticker) for ticker in val_tickers]\n",
    "    \n",
    "    ret_texts = get_texts(val_ciks, val_tickers) \n",
    "\n",
    "    val_pred = []\n",
    "    for doc in tqdm(ret_texts[\"docs\"]):\n",
    "        ret = get_count(doc, df_topk[[\"word\", \"isGood\"]], 2, 3)\n",
    "\n",
    "        # if ret[\"good_count\"] - ret[\"bad_count\"] > 0:\n",
    "        #     val_pred.append(1)\n",
    "        # else:\n",
    "        #     val_pred.append(0)\n",
    "        if ret[\"score\"] > 0:\n",
    "            val_pred.append(1)\n",
    "        else:\n",
    "            val_pred.append(0)\n",
    "    \n",
    "    print(\"val_pred: {}\".format(val_pred))\n",
    "    \n",
    "    return val_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation\n",
    "(Bigram and Trigram dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciks2ticker = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ciks(tickers):\n",
    "    ciks = []\n",
    "\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # for a given ticker, find its cik number through th ticker library\n",
    "            cik = get_cik(ticker)\n",
    "            ciks.append(cik)\n",
    "            ciks2ticker[cik] = ticker\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return ciks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tickers(ciks):\n",
    "    return [ciks2ticker[cik] for cik in ciks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dict_goodvbad(good_ticker, bad_ticker, score_type, sector=None, rerun=False):\n",
    "    \"\"\"\n",
    "    Train dictionary based on method 2: Good versus Bad. Refer to Section 3.4 of the paper\n",
    "    Input:\n",
    "        good_ticker (list): A list of good companies' tickers\n",
    "        bad_ticker (list): A list of bad companies' tickers\n",
    "        score_type (str): Specify the the score_type\n",
    "        sector (str): Specify the sector name; If none, then build non sector-specific dictionary\n",
    "        rerun (bool): If rerun == True, retrain a dictionary even if a dictionary already exists\n",
    "    Return:\n",
    "        df (DataFrame): A dictionary with index being words/bigrams and an associated score\n",
    "    \"\"\"\n",
    "    if sector:\n",
    "        path = os.path.join(\"data\", \"goodvbad\", \"train\", \"{}_{}.csv\".format(sector, score_type))\n",
    "    else:\n",
    "        path = os.path.join(\"data\", \"goodvbad\", \"train\", \"{}.csv\".format(score_type))\n",
    "    \n",
    "    if not rerun and os.path.exists(path):\n",
    "        dict_goodvbad = pd.read_csv(path, index_col=0)\n",
    "    else:\n",
    "        if sector:\n",
    "            print(\"Train {} {}\".format(sector, score_type))\n",
    "        else:\n",
    "            print(\"Train {}\".format(score_type))\n",
    "\n",
    "        good_cik = get_ciks(good_ticker)\n",
    "        bad_cik = get_ciks(bad_ticker)\n",
    "\n",
    "        good_ticker = get_tickers(good_cik)\n",
    "        bad_ticker = get_tickers(bad_cik)\n",
    "\n",
    "        ret_good = get_texts(good_cik, good_ticker)\n",
    "        ret_bad = get_texts(bad_cik, bad_ticker)\n",
    "\n",
    "        good_docs = ret_good[\"docs\"]\n",
    "        bad_docs = ret_bad[\"docs\"]\n",
    "\n",
    "        n_min = 2\n",
    "        n_max = 3\n",
    "        cv = CountVectorizer(max_df=0.7, stop_words=stop_words, max_features=200, ngram_range=(n_min, n_max))\n",
    "        word_count_vector = cv.fit_transform(good_docs + bad_docs)\n",
    "        count_feature = word_count_vector.toarray().sum(axis=0)\n",
    "        feature_names = cv.get_feature_names()\n",
    "\n",
    "        \n",
    "        d = {\"count\": [], \"good_nums\": [], \"bad_nums\": [], \"isGood\": []}\n",
    "\n",
    "        for feature_idx, word in enumerate(feature_names):\n",
    "            # good_sum = bad_sum = good_num = bad_num = 0\n",
    "            good_num = bad_num = 0\n",
    "\n",
    "            for i, doc_set in enumerate(good_docs):\n",
    "                if word in doc_set:\n",
    "                    good_num += 1\n",
    "            for i, doc_set in enumerate(bad_docs):\n",
    "                if word in doc_set:\n",
    "                    bad_num += 1\n",
    "            \n",
    "            d[\"count\"].append(count_feature[feature_idx])\n",
    "            d[\"good_nums\"].append(good_num)\n",
    "            d[\"bad_nums\"].append(bad_num)\n",
    "\n",
    "        diff = np.array(d[\"good_nums\"]) - np.array(d[\"bad_nums\"])\n",
    "        \n",
    "        alpha = 0.3\n",
    "        upper_score = np.quantile(diff, 1 - alpha)\n",
    "        lower_score = np.quantile(diff, alpha)\n",
    "        d[\"isGood\"] = np.where(diff > upper_score, 1, 0) + np.where(diff < lower_score, -1, 0)\n",
    "\n",
    "        dict_goodvbad = pd.DataFrame(d, index=feature_names)\n",
    "        dict_goodvbad.to_csv(path)\n",
    "\n",
    "    return dict_goodvbad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dict_tfidf(good_ticker, bad_ticker, score_type, sector=None, rerun=False):\n",
    "    \"\"\"\n",
    "    Train dictionary based on method 1: Good versus Bad. Refer to Section 3.3 of the paper\n",
    "    Input:\n",
    "        good_ticker (list): A list of good companies' tickers\n",
    "        bad_ticker (list): A list of bad companies' tickers\n",
    "        score_type (str): Specify the the score_type\n",
    "        sector (str): Specify the sector name; If none, then build non sector-specific dictionary\n",
    "        rerun (bool): If rerun == True, retrain a dictionary even if a dictionary already exists\n",
    "    Return:\n",
    "        df (DataFrame): A dictionary with index being words/bigrams and an associated score\n",
    "    \"\"\"\n",
    "    if sector:\n",
    "        path = os.path.join(\"data\", \"tfidf_scores\", \"train\", \"{}_{}.csv\".format(sector, score_type))\n",
    "    else:\n",
    "        path = os.path.join(\"data\", \"tfidf_scores\", \"train\", \"{}.csv\".format(score_type))\n",
    "\n",
    "    if not rerun and os.path.exists(path):\n",
    "        df_tfidf = pd.read_csv(path, index_col=0)\n",
    "    else:\n",
    "        if sector:\n",
    "            print(\"Train {} {}\".format(sector, score_type))\n",
    "        else:\n",
    "            print(\"Train {}\".format(score_type))\n",
    "        \n",
    "        tickers = good_ticker + bad_ticker\n",
    "        ciks = get_ciks(tickers)\n",
    "        tickers = get_tickers(ciks)\n",
    "\n",
    "        esgs = df_esg_score[df_esg_score[\"Company\"].isin(tickers)][[\"Company\", \"socialScore\", \"governanceScore\", \"environmentScore\"]]\n",
    "\n",
    "        ret = get_texts(ciks, tickers)\n",
    "        docs = ret[\"docs\"]\n",
    "\n",
    "        cv = CountVectorizer(max_df=0.8, stop_words=stop_words, max_features=1000)\n",
    "        word_count_vector = cv.fit_transform(docs)\n",
    "\n",
    "        tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "        tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "        feature_names = cv.get_feature_names()\n",
    "\n",
    "        df_doc_word = pd.DataFrame(columns=feature_names, index=tickers)\n",
    "\n",
    "        for i, ticker in tqdm(enumerate(tickers)):\n",
    "            tf_idf_vector = tfidf_transformer.transform(cv.transform([docs[i]]))\n",
    "            \n",
    "            coo_matrix = tf_idf_vector.tocoo()\n",
    "            # coo_matrix: A sparse matrix in which coo_matrix.col stores word_idx, coo_matrix.data stores tfidf score\n",
    "            \n",
    "            tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "            for word_idx, tfidf in tuples:\n",
    "                df_doc_word.at[ticker, feature_names[word_idx]] = tfidf\n",
    "\n",
    "        df_doc_word = df_doc_word.iloc[:, df_doc_word.columns.isin(words.words())]\n",
    "        df_doc_word = df_doc_word.fillna(0)\n",
    "\n",
    "        feature_names = [name for name in feature_names if name in words.words()]\n",
    "\n",
    "        df_tfidf = pd.DataFrame(columns=[\"{}_beta\".format(score_type)], index=feature_names)\n",
    "\n",
    "        # for typ in [\"social\", \"governance\", \"environment\"]:\n",
    "        score = esgs[score_type]\n",
    "        slopes = []\n",
    "        \n",
    "        for word in feature_names:\n",
    "            tfidfs = df_doc_word[word].values.astype(float)\n",
    "            slope, intercept, *_ = linregress(tfidfs, score)\n",
    "            slopes.append(slope)\n",
    "        df_tfidf[\"{}_beta\".format(score_type)] = slopes\n",
    "\n",
    "        cols = df_tfidf.columns\n",
    "        alpha = 0.3\n",
    "\n",
    "        for col in cols: \n",
    "            betas = df_tfidf[col]\n",
    "            score_type = col.split('_')[0]\n",
    "            \n",
    "            upper_score = np.quantile(betas, 1 - alpha)\n",
    "            lower_score = np.quantile(betas, alpha)\n",
    "            \n",
    "            is_good = np.where(betas < lower_score, 1, 0) + np.where(betas > upper_score, -1, 0)\n",
    "            \n",
    "            df_tfidf[\"isGood\"] = is_good\n",
    "\n",
    "        df_tfidf.to_csv(path)\n",
    "    return df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_tfidf(df_tfidf, val_tickers):\n",
    "    \"\"\" Perform the validation step\n",
    "    The validation rationale: Companies whose scores are in upper group are considered \"bad\" companies and the corresponding val_true = 1; 0 otherwise (in lower group, which is considered a good company)\n",
    "    Input:\n",
    "        df_topk (pd.DataFrame): containes the sector specific dict\n",
    "        val_tickers (list): A list of tickers to be validated\n",
    "    Return:\n",
    "        val_pred (list): A list of validation prediction\n",
    "    \"\"\"\n",
    "    df_tfidf[\"word\"] = df_tfidf.index\n",
    "    \n",
    "    # 1 if good_nums - bad_nums > threshold; -1 if good_nums - bad_nums < -threshold; 0 otherwise\n",
    "    val_ciks = [get_cik(ticker) for ticker in val_tickers]\n",
    "    \n",
    "    ret_texts = get_texts(val_ciks, val_tickers)\n",
    "\n",
    "    val_pred = []\n",
    "    for i, doc in tqdm(enumerate(ret_texts[\"docs\"])):\n",
    "        ret = get_count(doc, df_tfidf[[\"word\", \"isGood\"]])\n",
    "        \n",
    "        if ret[\"good_count\"] - ret[\"bad_count\"] > 0:\n",
    "            val_pred.append(1)\n",
    "        else:\n",
    "            val_pred.append(0)\n",
    "    \n",
    "    print(\"val_pred: {}\".format(val_pred))\n",
    "    \n",
    "    return val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_report(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    prec = cm[1][1]/(cm[1][1]+cm[1][0])\n",
    "    rec = cm[1][1]/(cm[1][1]+cm[0][1])\n",
    "    acc = (cm[1][1] + cm[0][0]) /(cm[1][0]+cm[0][1] + cm[1][1] + cm[0][0])\n",
    "\n",
    "    print(\"precision: \\n{}\".format(prec))\n",
    "    print(\"recall: \\n{}\".format(rec))\n",
    "    print(\"accuracy: \\n{}\".format(acc))\n",
    "    print(\"Confusion Matrix: \\n{}\".format(cm))\n",
    "\n",
    "    return {\"cm\": cm, \"precision\": prec, \"recall\": rec, \"accuracy\": acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors = [\"Consumer Cyclical\"]     #['Consumer Cyclical', 'Energy', 'Industrials', 'Healthcare', 'Basic Materials', 'Consumer Defensive', 'Utilities', 'Technology', 'Financial Services', 'Communication Services', 'Real Estate']\n",
    "score_types = [\"governanceScore\", \"environmentScore\", \"socialScore\"]       #[\"governanceScore\", \"environmentScore\", \"socialScore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_precision = pd.DataFrame(index=sectors, columns=score_types)\n",
    "df_val_recall = pd.DataFrame(index=sectors, columns=score_types)\n",
    "df_val_accuracy = pd.DataFrame(index=sectors, columns=score_types)\n",
    "cms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "approach = \"goodvbad\" # (\"tfidf\", \"goodvbad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dicts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = pd.DataFrame(index=['accuracy', 'precision', 'recall', 'training_set'])\n",
    "validation_results = pd.DataFrame(index=['accuracy', 'precision', 'recall', 'training_set', 'validation_set'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Validatation of dictionary based on score types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score_type in score_types:\n",
    "    print(\"{}\".format(score_type))\n",
    "    esgs = df_esg_score[[\"Company\", \"socialScore\", \"governanceScore\", \"environmentScore\"]]\n",
    "    score = esgs[score_type]\n",
    "    alpha = 0.3\n",
    "    upper_score = np.quantile(score, 1 - alpha)\n",
    "    lower_score = np.quantile(score, alpha)\n",
    "\n",
    "    good_companies = list(esgs[esgs[score_type] < lower_score][\"Company\"].values)\n",
    "    bad_companies = list(esgs[esgs[score_type] > upper_score][\"Company\"].values)\n",
    "            \n",
    "    #training set\n",
    "    train_good = random.sample(list(good_companies), int(len(good_companies) * 0.7))\n",
    "    train_bad = random.sample(list(bad_companies), int(len(bad_companies) * 0.7))\n",
    "\n",
    "    if approach == \"goodvbad\":\n",
    "        df_dict = train_dict_goodvbad(train_good, train_bad, score_type, sector=None, rerun=False)\n",
    "        \n",
    "        if \"word\" not in df_dict.columns:\n",
    "            df_dict[\"word\"] = df_dict.index.to_numpy()\n",
    "\n",
    "    if approach == \"tfidf\":\n",
    "        df_dict = train_dict_tfidf(train_good, train_bad, score_type, sector=None, rerun=False)\n",
    "    \n",
    "    all_dicts.append(df_dict)\n",
    "\n",
    "    #validation set\n",
    "    validate_good = [ticker for ticker in good_companies if ticker not in train_good]\n",
    "    validate_bad = [ticker for ticker in bad_companies if ticker not in train_bad]\n",
    "\n",
    "    val_tickers = validate_good + validate_bad\n",
    "    \n",
    "    # val_pred = validation_tfidf(df_dict, val_tickers)   # df is the dictionary with good_num, bad_num, diff\n",
    "    val_pred = validation(df_dict, val_tickers, score_scheme=\"ratio_idf\")\n",
    "    val_true = [1] * len(validate_good) + [0] * len(validate_bad)\n",
    "\n",
    "    val_performance = get_report(val_true, val_pred)\n",
    "\n",
    "    cm = val_performance[\"cm\"]\n",
    "    prec = cm[1][1]/(cm[1][1]+cm[0][1])\n",
    "    rec = cm[1][1]/(cm[1][1]+cm[1][0])\n",
    "    tpr = cm[1][1]/(cm[1][1]+cm[0][1])\n",
    "    tnr = cm[0][0]/(cm[0][0]+cm[1][0])\n",
    "    acc = (cm[1][1] + cm[0][0]) /(cm[1][0]+cm[0][1] + cm[1][1] + cm[0][0])\n",
    "\n",
    "    num_train = len(list(train_good) + list(train_bad))\n",
    "    num_validate = len(validate_good + validate_bad)\n",
    "\n",
    "    training_results.at['precision', score_type] = prec\n",
    "    training_results.at['recall', score_type] = rec\n",
    "    training_results.at['accuracy', score_type] = acc\n",
    "    training_results.at['training_set', score_type] = num_train\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    129\n",
       " 1     56\n",
       "-1     15\n",
       "Name: isGood, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict[\"isGood\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Validatation of dictionary based on full ESG scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>socialScore</th>\n",
       "      <th>governanceScore</th>\n",
       "      <th>environmentScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LEG</td>\n",
       "      <td>17.19</td>\n",
       "      <td>11.23</td>\n",
       "      <td>20.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COG</td>\n",
       "      <td>14.01</td>\n",
       "      <td>9.28</td>\n",
       "      <td>23.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GE</td>\n",
       "      <td>15.72</td>\n",
       "      <td>11.98</td>\n",
       "      <td>15.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MRO</td>\n",
       "      <td>10.27</td>\n",
       "      <td>8.70</td>\n",
       "      <td>23.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CVX</td>\n",
       "      <td>10.67</td>\n",
       "      <td>10.21</td>\n",
       "      <td>20.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>HAS</td>\n",
       "      <td>5.33</td>\n",
       "      <td>5.07</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>AVB</td>\n",
       "      <td>2.92</td>\n",
       "      <td>4.40</td>\n",
       "      <td>2.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>PLD</td>\n",
       "      <td>3.38</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>RHI</td>\n",
       "      <td>5.76</td>\n",
       "      <td>3.56</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>CDW</td>\n",
       "      <td>3.29</td>\n",
       "      <td>3.45</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>327 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Company  socialScore  governanceScore  environmentScore\n",
       "0       LEG        17.19            11.23             20.38\n",
       "1       COG        14.01             9.28             23.39\n",
       "2        GE        15.72            11.98             15.65\n",
       "3       MRO        10.27             8.70             23.76\n",
       "4       CVX        10.67            10.21             20.29\n",
       "..      ...          ...              ...               ...\n",
       "442     HAS         5.33             5.07              0.05\n",
       "444     AVB         2.92             4.40              2.95\n",
       "445     PLD         3.38             4.23              2.44\n",
       "446     RHI         5.76             3.56              0.07\n",
       "448     CDW         3.29             3.45              2.40\n",
       "\n",
       "[327 rows x 4 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train governanceScore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0000100493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0000100493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.61s/it]\n",
      "10it [00:39,  3.99s/it]\n",
      "0it [00:00, ?it/s]\n",
      "/Users/luckywang/Documents/Document/Course Material/Fall 2021/esg_nlp/venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['10'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             count  good_nums  bad_nums  isGood  \\\n",
      "accompanying note integral    1006          4         0       0   \n",
      "accrued liability             1058          7         0       1   \n",
      "approximately percent         1325          6         0       1   \n",
      "asset retirement obligation   1762          7         0       1   \n",
      "average price                 1031          7         0       1   \n",
      "...                            ...        ...       ...     ...   \n",
      "taxonomy extension             970          5         0       0   \n",
      "undeveloped reserve            939          4         0       0   \n",
      "unproved property             1075          4         0       0   \n",
      "well cost                     1076          5         0       0   \n",
      "working interest              1768          5         0       0   \n",
      "\n",
      "                                                    word  \n",
      "accompanying note integral    accompanying note integral  \n",
      "accrued liability                      accrued liability  \n",
      "approximately percent              approximately percent  \n",
      "asset retirement obligation  asset retirement obligation  \n",
      "average price                              average price  \n",
      "...                                                  ...  \n",
      "taxonomy extension                    taxonomy extension  \n",
      "undeveloped reserve                  undeveloped reserve  \n",
      "unproved property                      unproved property  \n",
      "well cost                                      well cost  \n",
      "working interest                        working interest  \n",
      "\n",
      "[200 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0001585364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CIK 0001585364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.44s/it]\n",
      "5it [00:54, 10.97s/it]\n",
      " 20%|██        | 1/5 [00:12<00:49, 12.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-90]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:17<00:23,  7.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[489]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:24<00:15,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[911]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [01:28<00:30, 30.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19351]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:32<00:00, 30.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20929]\n",
      "val_pred: [0, 1, 1, 1, 1]\n",
      "precision: \n",
      "0.8\n",
      "recall: \n",
      "1.0\n",
      "accuracy: \n",
      "0.8\n",
      "Confusion Matrix: \n",
      "[[0 0]\n",
      " [1 4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "score = df_esg_score[\"totalEsg\"]\n",
    "alpha = 0.3\n",
    "upper_score = np.quantile(score, 1 - alpha)\n",
    "lower_score = np.quantile(score, alpha)\n",
    "\n",
    "good_companies = list(esgs[esgs[score_type] < lower_score][\"Company\"].values)\n",
    "bad_companies = list(esgs[esgs[score_type] > upper_score][\"Company\"].values)\n",
    "        \n",
    "#training set\n",
    "train_good = random.sample(list(good_companies), int(len(good_companies) * 0.7))\n",
    "train_bad = random.sample(list(bad_companies), int(len(bad_companies) * 0.7))\n",
    "\n",
    "if approach == \"goodvbad\":\n",
    "    df_dict = train_dict_goodvbad(train_good, train_bad, score_type, sector=None, rerun=True)\n",
    "    \n",
    "    if \"word\" not in df_dict.columns:\n",
    "        df_dict[\"word\"] = df_dict.index.to_numpy()\n",
    "\n",
    "if approach == \"tfidf\":\n",
    "    df_dict = train_dict_tfidf(train_good, train_bad, score_type, sector=None, rerun=True)\n",
    "\n",
    "all_dicts.append(df_dict)\n",
    "\n",
    "#validation set\n",
    "validate_good = [ticker for ticker in good_companies if ticker not in train_good]\n",
    "validate_bad = [ticker for ticker in bad_companies if ticker not in train_bad]\n",
    "\n",
    "val_tickers = validate_good + validate_bad\n",
    "\n",
    "# val_pred = validation_tfidf(df_dict, val_tickers)   # df is the dictionary with good_num, bad_num, diff\n",
    "val_pred = validation(df_dict, val_tickers, score_scheme=\"ratio_idf\")\n",
    "val_true = [1] * len(validate_good) + [0] * len(validate_bad)\n",
    "\n",
    "val_performance = get_report(val_true, val_pred)\n",
    "\n",
    "cm = val_performance[\"cm\"]\n",
    "prec = cm[1][1]/(cm[1][1]+cm[0][1])\n",
    "rec = cm[1][1]/(cm[1][1]+cm[1][0])\n",
    "tpr = cm[1][1]/(cm[1][1]+cm[0][1])\n",
    "tnr = cm[0][0]/(cm[0][0]+cm[1][0])\n",
    "acc = (cm[1][1] + cm[0][0]) /(cm[1][0]+cm[0][1] + cm[1][1] + cm[0][0])\n",
    "\n",
    "num_train = len(list(train_good) + list(train_bad))\n",
    "num_validate = len(validate_good + validate_bad)\n",
    "\n",
    "training_results.at['precision', 'ESG'] = prec\n",
    "training_results.at['recall', 'ESG'] = rec\n",
    "training_results.at['accuracy', 'ESG'] = acc\n",
    "training_results.at['training_set', 'ESG'] = num_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Validatation of sector and score type specific dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = pd.DataFrame()\n",
    "recall = pd.DataFrame()\n",
    "accuracy = pd.DataFrame()\n",
    "true_pos = pd.DataFrame()\n",
    "true_neg = pd.DataFrame()\n",
    "training_comp = pd.DataFrame()\n",
    "validate_comp = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sector in sectors:\n",
    "    for score_type in score_types:\n",
    "        print(\"{} {}\".format(sector, score_type))\n",
    "        esgs = df_esg_score[df_esg_score[\"sector\"] == sector][[\"Company\", \"socialScore\", \"governanceScore\", \"environmentScore\"]]\n",
    "        score = esgs[score_type]\n",
    "        alpha = 0.3\n",
    "        upper_score = np.quantile(score, 1 - alpha)\n",
    "        lower_score = np.quantile(score, alpha)\n",
    "\n",
    "        good_companies = list(esgs[esgs[score_type] < lower_score][\"Company\"].values)[:5]\n",
    "        bad_companies = list(esgs[esgs[score_type] > upper_score][\"Company\"].values)[:5]\n",
    "                \n",
    "        #training set\n",
    "        train_good = random.sample(list(good_companies), int(len(good_companies) * 0.7))\n",
    "        train_bad = random.sample(list(bad_companies), int(len(bad_companies) * 0.7))\n",
    "\n",
    "        if approach == \"goodvbad\":\n",
    "            df_dict = train_dict_goodvbad(train_good, train_bad, score_type, sector=None, rerun=True)\n",
    "            \n",
    "            if \"word\" not in df_dict.columns:\n",
    "                df_dict[\"word\"] = df_dict.index.to_numpy()\n",
    "\n",
    "        if approach == \"tfidf\":\n",
    "            df_dict = train_dict_tfidf(train_good, train_bad, score_type, sector=None, rerun=True)\n",
    "        \n",
    "        all_dicts.append(df_dict)\n",
    "\n",
    "        #validation set\n",
    "        validate_good = [ticker for ticker in good_companies if ticker not in train_good]\n",
    "        validate_bad = [ticker for ticker in bad_companies if ticker not in train_bad]\n",
    "\n",
    "        val_tickers = validate_good + validate_bad\n",
    "        \n",
    "        # val_pred = validation_tfidf(df_dict, val_tickers)   # df is the dictionary with good_num, bad_num, diff\n",
    "        val_pred = validation(df_dict, val_tickers, score_scheme=\"ratio\")\n",
    "        val_true = [1] * len(validate_good) + [0] * len(validate_bad)\n",
    "\n",
    "        val_performance = get_report(val_true, val_pred)\n",
    "\n",
    "        cm = val_performance[\"cm\"]\n",
    "        prec = cm[1][1]/(cm[1][1]+cm[0][1])\n",
    "        rec = cm[1][1]/(cm[1][1]+cm[1][0])\n",
    "        tpr = cm[1][1]/(cm[1][1]+cm[0][1])\n",
    "        tnr = cm[0][0]/(cm[0][0]+cm[1][0])\n",
    "        acc = (cm[1][1] + cm[0][0]) /(cm[1][0]+cm[0][1] + cm[1][1] + cm[0][0])\n",
    "\n",
    "        num_train = len(list(train_good) + list(train_bad))\n",
    "        num_validate = len(validate_good + validate_bad)\n",
    "\n",
    "        training_comp.at[sector, score_type] = num_train\n",
    "        validate_comp.at[sector, score_type] = num_validate\n",
    "        \n",
    "        precision.at[sector, score_type] = prec\n",
    "        recall.at[sector, score_type] = rec\n",
    "        accuracy.at[sector, score_type] = acc\n",
    "        true_pos.at[sector, score_type] = tpr\n",
    "        true_neg.at[sector, score_type] = tnr\n",
    "        accuracy.at[sector, score_type] = acc\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bcf325ffca4e7352bcfac699b7999bb028ddd94d8390136137b75043a4ee01b0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
