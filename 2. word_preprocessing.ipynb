{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/luckywang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/luckywang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/luckywang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Importing libraries you need to install\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from string import punctuation\n",
    "\n",
    "# Import yfinance and pandas_datareader\n",
    "from pandas_datareader import data as pdr\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "\n",
    "# import yfinance as yf \n",
    "\n",
    "# Override function to store data we get\n",
    "# yf.pdr_override()\n",
    "\n",
    "# Import nltk for first step extracting words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Set up stop_words from nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words |= {'10-k', 'form', 'table', 'contents', 'united', 'states', 'securities', 'exchange', 'commission'}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "\"\"\"\n",
    "this is where different from version 1\n",
    "\"\"\"\n",
    "#import libraries for n-gram counting\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now we have all the txts stored in the file:\n",
    "'./data/10k/[cik]/rawtext/[cik]_[date]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we can make a dictionary to store all the data needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luckywang/Documents/Document/Course Material/Fall 2021/esg_nlp/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# read the ticker library of all the tikers into ticker_library\n",
    "ticker_library = pd.read_csv(os.path.join(\"data\", \"tickers.csv\"))\n",
    "\n",
    "# read the sp500 components into ticker_selected, 'name' is the company name and ticker is company's ticker\n",
    "ticker_selected = pd.read_csv(os.path.join(\"data\", \"SP500_component_stocks.csv\"), header = None)\n",
    "ticker_selected.columns = ['name','ticker']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001090872</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000006201</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001158449</td>\n",
       "      <td>AAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000320193</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001551152</td>\n",
       "      <td>ABBV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cik ticker\n",
       "0  0001090872      A\n",
       "1  0000006201    AAL\n",
       "2  0001158449    AAP\n",
       "3  0000320193   AAPL\n",
       "4  0001551152   ABBV"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a ticker_cik_df dataframe to store ticker and its cik number\n",
    "ticker_cik_df = pd.DataFrame()\n",
    "\n",
    "# store all the tickers in a ticker_list\n",
    "ticker_list = ticker_selected.ticker\n",
    "\n",
    "# build a list cik_list for cik\n",
    "cik_list = []\n",
    "\n",
    "for ticker in ticker_list:    \n",
    "    try:\n",
    "        # for a given ticker, find its cik number through th ticker library\n",
    "        cik_list.append(list(ticker_library[ticker_library.ticker == ticker].secfilings)[0][-10:])\n",
    "        \n",
    "    except:\n",
    "        # if could not find cik, give it a empty cik\n",
    "        cik_list.append('')\n",
    "\n",
    "# write cik_list and ticker_list to the dataframe ticker_cik_df\n",
    "ticker_cik_df['cik'] = cik_list\n",
    "ticker_cik_df['ticker'] = ticker_list\n",
    "\n",
    "# delete the tickers with empty cik number\n",
    "ticker_cik_df = ticker_cik_df[ticker_cik_df['cik'] != '']\n",
    "\n",
    "# display a sample of ticker_cik_df\n",
    "ticker_cik_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIK2TICKER = {row[\"cik\"]: row[\"ticker\"] for _, row in ticker_cik_df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "listtickers = ['AMZN','BBY','BKNG','MCD','EBAY','F','HD','TGT','WHR','JPM','SIVB','CFG','C','ALL','IVZ','ETFC','MET','PFG','CBOE',\n",
    "              'CTL','IPG','VIAC','NFLX','CHTR','FB','TWTR','NWSA','FOXA','AMD','INTC','AAPL','LRCX','MSFT','NLOK','CTSH','ADS',\n",
    "              'WU','PAYC','ABT','CVS','PFE','JNJ','BIIB','INCY','HSIC','WAT','ALGN','EW']\n",
    "\n",
    "ticker_cik_sample = pd.DataFrame()\n",
    "\n",
    "for ticker in listtickers:\n",
    "    ticker_cik_sample = ticker_cik_sample.append(ticker_cik_df[ticker_cik_df['ticker'] == ticker])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list = ticker_cik_sample[\"cik\"].values\n",
    "ticker_list = ticker_cik_sample[\"ticker\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store data using dictionary\n",
    "all_data = {}\n",
    "\n",
    "# set the key of dictionary as ticker\n",
    "for cik, ticker in zip(cik_list, ticker_list):\n",
    "    \n",
    "    # set the value of tikcer as a dict\n",
    "    all_data[ticker] = {}\n",
    "\n",
    "    # set the dict data[ticker] \n",
    "    all_data[ticker]['cik'] = cik\n",
    "    all_data[ticker]['10ks'] = {}\n",
    "    all_data[ticker]['10qs'] = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cik': '0000049826', '10ks': {}, '10qs': {}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['ITW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_10k = './data/10k/'\n",
    "dir_10q = './data/10q/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "print(len(cik_list))\n",
    "print(len(ticker_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to remove punctuations if a given word ended with a punctuation\n",
    "def remove_punct(string):\n",
    "    return re.sub(r\"[{}]+\".format(punctuation), \"\", string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for filtering words\n",
    "def is_words(string):\n",
    "    return bool(re.match(r'^[a-z\\']+$', string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_cik_texts(cik, filetype):\n",
    "    \"\"\"\n",
    "    Collect all the texts related to given `cik` with given filetype and \n",
    "    return a single string\n",
    "    \"\"\"\n",
    "    \n",
    "    assert filetype in (\"10k\", \"10q\")\n",
    "    \n",
    "    cik_dir = os.path.join(\"data\", filetype, cik)\n",
    "    rawtext_dir = os.path.join(cik_dir, \"rawtext\")\n",
    "    # goes into the directory to find the path for txtfiles\n",
    "    try:\n",
    "        all_files = os.listdir(rawtext_dir)\n",
    "    except:\n",
    "        print(\"No such dir\")\n",
    "    \n",
    "    texts = \"\"\n",
    "    for file in all_files:\n",
    "        with open(os.path.join(\"data\", filetype, cik, \"rawtext\", file), encoding = \"utf8\") as f:\n",
    "            string_temp = f.read().lower()\n",
    "            texts += string_temp\n",
    "    \n",
    "    texts = remove_punct(texts)\n",
    "    \n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    # Tokenize texts, remove stopwords and numbers, and keep only the relevant words\n",
    "    # Then lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in nltk.word_tokenize(texts) if token not in stop_words and is_words(token)]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts2counter(texts):\n",
    "    counter = Counter(tokens)\n",
    "    pkl_path = os.path.join(cik_dir, \"pickle\")\n",
    "    if not os.path.isdir(pkl_path):\n",
    "        os.mkdir(pkl_path)\n",
    "    \n",
    "    with open(os.path.join(pkl_path, 'token_counter.pkl'), 'wb') as f:\n",
    "        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "        pickle.dump(counter, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return counter\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 45/45 [2:50:09<00:00, 226.87s/it]\n"
     ]
    }
   ],
   "source": [
    "# './data/10k/[cik]/rawtext/[cik]_[date]'\n",
    "docs = []\n",
    "tickers = []\n",
    "\n",
    "for cik in tqdm(cik_list):\n",
    "    tickers.append(CIK2TICKER[cik])\n",
    "    texts = \"\"\n",
    "    for filetype in [\"10k\", \"10q\"]:\n",
    "        text = aggregate_cik_texts(cik, filetype)\n",
    "        texts += preprocess(text)\n",
    "        # counter = texts2counter(texts)\n",
    "    docs.append(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luckywang/Documents/Document/Course Material/Fall 2021/esg_nlp/venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['10'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(max_df=0.8, stop_words=stop_words, max_features=10000)\n",
    "word_count_vector = cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results = {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]] = score_vals[idx]\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "# tf_idf_vector = tfidf_transformer.transform(word_count_vector)\n",
    "# tf_idf_vector = tfidf_transformer.transform(cv.transform([docs[3]]))\n",
    "# tf_idf_vector = tfidf_transformer.transform(cv.transform(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = []\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    tf_idf_vector = tfidf_transformer.transform(cv.transform([docs[i]]))\n",
    "    # you only needs to do this once, this is a mapping of index to \n",
    "    feature_names = cv.get_feature_names()\n",
    "\n",
    "    sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "    # extract only the top n; n here is 10\n",
    "    keyword = extract_topn_from_vector(feature_names, sorted_items, 30)\n",
    "    \n",
    "    keywords.append(keyword)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_top_k = defaultdict(list)\n",
    "dict_top_k[\"tickers\"] = tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keyword in keywords:\n",
    "    for i, word in enumerate(keyword.keys()):\n",
    "        dict_top_k[\"word_{}\".format(i)].append(word)\n",
    "        dict_top_k[\"tfidf_{}\".format(i)].append(keyword[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_k_word = pd.DataFrame(dict_top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_0</th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>word_3</th>\n",
       "      <th>word_4</th>\n",
       "      <th>word_5</th>\n",
       "      <th>word_6</th>\n",
       "      <th>word_7</th>\n",
       "      <th>word_8</th>\n",
       "      <th>word_9</th>\n",
       "      <th>...</th>\n",
       "      <th>word_20</th>\n",
       "      <th>word_21</th>\n",
       "      <th>word_22</th>\n",
       "      <th>word_23</th>\n",
       "      <th>word_24</th>\n",
       "      <th>word_25</th>\n",
       "      <th>word_26</th>\n",
       "      <th>word_27</th>\n",
       "      <th>word_28</th>\n",
       "      <th>word_29</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tickers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>fulfillment</td>\n",
       "      <td>amazoncom</td>\n",
       "      <td>peacs</td>\n",
       "      <td>shipping</td>\n",
       "      <td>marketable</td>\n",
       "      <td>amazon</td>\n",
       "      <td>merchandise</td>\n",
       "      <td>ecommerce</td>\n",
       "      <td>aws</td>\n",
       "      <td>equitymethod</td>\n",
       "      <td>...</td>\n",
       "      <td>szkutak</td>\n",
       "      <td>internationallyfocused</td>\n",
       "      <td>shipment</td>\n",
       "      <td>absolute</td>\n",
       "      <td>harm</td>\n",
       "      <td>wrongdoing</td>\n",
       "      <td>infringe</td>\n",
       "      <td>subordinated</td>\n",
       "      <td>toysruscom</td>\n",
       "      <td>investees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BBY</th>\n",
       "      <td>sga</td>\n",
       "      <td>merchandise</td>\n",
       "      <td>magnolia</td>\n",
       "      <td>appliance</td>\n",
       "      <td>musicland</td>\n",
       "      <td>ar</td>\n",
       "      <td>debenture</td>\n",
       "      <td>squad</td>\n",
       "      <td>geek</td>\n",
       "      <td>electronics</td>\n",
       "      <td>...</td>\n",
       "      <td>television</td>\n",
       "      <td>debttocapitalization</td>\n",
       "      <td>consolidating</td>\n",
       "      <td>carphone</td>\n",
       "      <td>guarantor</td>\n",
       "      <td>revolving</td>\n",
       "      <td>auctionrate</td>\n",
       "      <td>auction</td>\n",
       "      <td>largeformat</td>\n",
       "      <td>gaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BKNG</th>\n",
       "      <td>hotel</td>\n",
       "      <td>reservation</td>\n",
       "      <td>travel</td>\n",
       "      <td>pricelinecom</td>\n",
       "      <td>bookingcom</td>\n",
       "      <td>booking</td>\n",
       "      <td>airline</td>\n",
       "      <td>ticket</td>\n",
       "      <td>accommodation</td>\n",
       "      <td>car</td>\n",
       "      <td>...</td>\n",
       "      <td>night</td>\n",
       "      <td>opentable</td>\n",
       "      <td>braddock</td>\n",
       "      <td>european</td>\n",
       "      <td>travelweb</td>\n",
       "      <td>agoda</td>\n",
       "      <td>gd</td>\n",
       "      <td>opaque</td>\n",
       "      <td>expedia</td>\n",
       "      <td>otcs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCD</th>\n",
       "      <td>restaurant</td>\n",
       "      <td>companyoperated</td>\n",
       "      <td>franchised</td>\n",
       "      <td>franchisees</td>\n",
       "      <td>mcdonalds</td>\n",
       "      <td>apmea</td>\n",
       "      <td>developmental</td>\n",
       "      <td>constant</td>\n",
       "      <td>systemwide</td>\n",
       "      <td>mcdonald</td>\n",
       "      <td>...</td>\n",
       "      <td>guest</td>\n",
       "      <td>nm</td>\n",
       "      <td>japan</td>\n",
       "      <td>occupancy</td>\n",
       "      <td>nutritional</td>\n",
       "      <td>conventional</td>\n",
       "      <td>franchising</td>\n",
       "      <td>reimaging</td>\n",
       "      <td>count</td>\n",
       "      <td>ieo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EBAY</th>\n",
       "      <td>paypal</td>\n",
       "      <td>ebay</td>\n",
       "      <td>harm</td>\n",
       "      <td>skype</td>\n",
       "      <td>paypals</td>\n",
       "      <td>merchant</td>\n",
       "      <td>gsi</td>\n",
       "      <td>ticket</td>\n",
       "      <td>stubhub</td>\n",
       "      <td>listing</td>\n",
       "      <td>...</td>\n",
       "      <td>tpv</td>\n",
       "      <td>processor</td>\n",
       "      <td>ebaycom</td>\n",
       "      <td>shopping</td>\n",
       "      <td>harmed</td>\n",
       "      <td>prohibit</td>\n",
       "      <td>counterfeit</td>\n",
       "      <td>european</td>\n",
       "      <td>licensure</td>\n",
       "      <td>email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>ford</td>\n",
       "      <td>automotive</td>\n",
       "      <td>sector</td>\n",
       "      <td>incomeloss</td>\n",
       "      <td>motor</td>\n",
       "      <td>dealer</td>\n",
       "      <td>securitization</td>\n",
       "      <td>pension</td>\n",
       "      <td>volvo</td>\n",
       "      <td>wholesale</td>\n",
       "      <td>...</td>\n",
       "      <td>warranty</td>\n",
       "      <td>rover</td>\n",
       "      <td>fce</td>\n",
       "      <td>vies</td>\n",
       "      <td>lincoln</td>\n",
       "      <td>veba</td>\n",
       "      <td>jaguar</td>\n",
       "      <td>hertz</td>\n",
       "      <td>operatingrelated</td>\n",
       "      <td>convertible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HD</th>\n",
       "      <td>hd</td>\n",
       "      <td>associate</td>\n",
       "      <td>merchandise</td>\n",
       "      <td>sga</td>\n",
       "      <td>expo</td>\n",
       "      <td>installation</td>\n",
       "      <td>carol</td>\n",
       "      <td>assortment</td>\n",
       "      <td>asr</td>\n",
       "      <td>omnibus</td>\n",
       "      <td>...</td>\n",
       "      <td>interline</td>\n",
       "      <td>shrink</td>\n",
       "      <td>nardelli</td>\n",
       "      <td>appliance</td>\n",
       "      <td>futurebuilder</td>\n",
       "      <td>plumbing</td>\n",
       "      <td>interconnected</td>\n",
       "      <td>blake</td>\n",
       "      <td>promulgated</td>\n",
       "      <td>atlanta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TGT</th>\n",
       "      <td>guest</td>\n",
       "      <td>sga</td>\n",
       "      <td>merchandise</td>\n",
       "      <td>pension</td>\n",
       "      <td>comparablestore</td>\n",
       "      <td>redcard</td>\n",
       "      <td>ebit</td>\n",
       "      <td>nonrecourse</td>\n",
       "      <td>postretirement</td>\n",
       "      <td>scovanner</td>\n",
       "      <td>...</td>\n",
       "      <td>linkbase</td>\n",
       "      <td>redcards</td>\n",
       "      <td>mervyns</td>\n",
       "      <td>marshall</td>\n",
       "      <td>minnesota</td>\n",
       "      <td>trc</td>\n",
       "      <td>supertarget</td>\n",
       "      <td>visa</td>\n",
       "      <td>collateralized</td>\n",
       "      <td>reacquired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHR</th>\n",
       "      <td>whirlpool</td>\n",
       "      <td>brazilian</td>\n",
       "      <td>befiex</td>\n",
       "      <td>indesit</td>\n",
       "      <td>appliance</td>\n",
       "      <td>pension</td>\n",
       "      <td>maytag</td>\n",
       "      <td>sundry</td>\n",
       "      <td>embraco</td>\n",
       "      <td>compressor</td>\n",
       "      <td>...</td>\n",
       "      <td>ipi</td>\n",
       "      <td>merloni</td>\n",
       "      <td>curtailment</td>\n",
       "      <td>harbor</td>\n",
       "      <td>raw</td>\n",
       "      <td>export</td>\n",
       "      <td>forwardsoptions</td>\n",
       "      <td>european</td>\n",
       "      <td>oilrelated</td>\n",
       "      <td>monetized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JPM</th>\n",
       "      <td>jpmorgan</td>\n",
       "      <td>chase</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>lendingrelated</td>\n",
       "      <td>banking</td>\n",
       "      <td>var</td>\n",
       "      <td>basel</td>\n",
       "      <td>deposit</td>\n",
       "      <td>collateral</td>\n",
       "      <td>client</td>\n",
       "      <td>...</td>\n",
       "      <td>stearns</td>\n",
       "      <td>servicing</td>\n",
       "      <td>nonaccrual</td>\n",
       "      <td>nm</td>\n",
       "      <td>msrs</td>\n",
       "      <td>chargeoff</td>\n",
       "      <td>lending</td>\n",
       "      <td>vies</td>\n",
       "      <td>tss</td>\n",
       "      <td>tier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word_0           word_1       word_2          word_3  \\\n",
       "tickers                                                              \n",
       "AMZN     fulfillment        amazoncom        peacs        shipping   \n",
       "BBY              sga      merchandise     magnolia       appliance   \n",
       "BKNG           hotel      reservation       travel    pricelinecom   \n",
       "MCD       restaurant  companyoperated   franchised     franchisees   \n",
       "EBAY          paypal             ebay         harm           skype   \n",
       "F               ford       automotive       sector      incomeloss   \n",
       "HD                hd        associate  merchandise             sga   \n",
       "TGT            guest              sga  merchandise         pension   \n",
       "WHR        whirlpool        brazilian       befiex         indesit   \n",
       "JPM         jpmorgan            chase     mortgage  lendingrelated   \n",
       "\n",
       "                  word_4        word_5          word_6       word_7  \\\n",
       "tickers                                                               \n",
       "AMZN          marketable        amazon     merchandise    ecommerce   \n",
       "BBY            musicland            ar       debenture        squad   \n",
       "BKNG          bookingcom       booking         airline       ticket   \n",
       "MCD            mcdonalds         apmea   developmental     constant   \n",
       "EBAY             paypals      merchant             gsi       ticket   \n",
       "F                  motor        dealer  securitization      pension   \n",
       "HD                  expo  installation           carol   assortment   \n",
       "TGT      comparablestore       redcard            ebit  nonrecourse   \n",
       "WHR            appliance       pension          maytag       sundry   \n",
       "JPM              banking           var           basel      deposit   \n",
       "\n",
       "                 word_8        word_9  ...     word_20  \\\n",
       "tickers                                ...               \n",
       "AMZN                aws  equitymethod  ...     szkutak   \n",
       "BBY                geek   electronics  ...  television   \n",
       "BKNG      accommodation           car  ...       night   \n",
       "MCD          systemwide      mcdonald  ...       guest   \n",
       "EBAY            stubhub       listing  ...         tpv   \n",
       "F                 volvo     wholesale  ...    warranty   \n",
       "HD                  asr       omnibus  ...   interline   \n",
       "TGT      postretirement     scovanner  ...    linkbase   \n",
       "WHR             embraco    compressor  ...         ipi   \n",
       "JPM          collateral        client  ...     stearns   \n",
       "\n",
       "                        word_21        word_22    word_23        word_24  \\\n",
       "tickers                                                                    \n",
       "AMZN     internationallyfocused       shipment   absolute           harm   \n",
       "BBY        debttocapitalization  consolidating   carphone      guarantor   \n",
       "BKNG                  opentable       braddock   european      travelweb   \n",
       "MCD                          nm          japan  occupancy    nutritional   \n",
       "EBAY                  processor        ebaycom   shopping         harmed   \n",
       "F                         rover            fce       vies        lincoln   \n",
       "HD                       shrink       nardelli  appliance  futurebuilder   \n",
       "TGT                    redcards        mervyns   marshall      minnesota   \n",
       "WHR                     merloni    curtailment     harbor            raw   \n",
       "JPM                   servicing     nonaccrual         nm           msrs   \n",
       "\n",
       "              word_25          word_26       word_27           word_28  \\\n",
       "tickers                                                                  \n",
       "AMZN       wrongdoing         infringe  subordinated        toysruscom   \n",
       "BBY         revolving      auctionrate       auction       largeformat   \n",
       "BKNG            agoda               gd        opaque           expedia   \n",
       "MCD      conventional      franchising     reimaging             count   \n",
       "EBAY         prohibit      counterfeit      european         licensure   \n",
       "F                veba           jaguar         hertz  operatingrelated   \n",
       "HD           plumbing   interconnected         blake       promulgated   \n",
       "TGT               trc      supertarget          visa    collateralized   \n",
       "WHR            export  forwardsoptions      european        oilrelated   \n",
       "JPM         chargeoff          lending          vies               tss   \n",
       "\n",
       "             word_29  \n",
       "tickers               \n",
       "AMZN       investees  \n",
       "BBY           gaming  \n",
       "BKNG            otcs  \n",
       "MCD              ieo  \n",
       "EBAY           email  \n",
       "F        convertible  \n",
       "HD           atlanta  \n",
       "TGT       reacquired  \n",
       "WHR        monetized  \n",
       "JPM             tier  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top_k_word = df_top_k_word.set_index(\"tickers\")\n",
    "df_top_k_word.filter(regex='word*', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kicked'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"kicked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'franchisees'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"franchisees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_k_word.to_csv(\"data/45_companies_top_30.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(pkl_path, 'token_counter.pkl'), 'rb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    c = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze 10K word with tfidf and bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is to compute the tf values for the words in 10ks and collect the overall word list for computing idf in the next step\n",
    "\"\"\"\n",
    "#count the number of 10K documents we have\n",
    "document_num_10k = 0\n",
    "\n",
    "#word list for 10k \n",
    "word_list_10k = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10ks']:\n",
    "        \n",
    "        #we have document for a given date, so add 1 for document_num_10k\n",
    "        document_num_10k += 1\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        all_data[ticker]['10ks'][date]['tf'] = Counter(all_data[ticker]['10ks'][date]['words'].split())\n",
    "        \n",
    "        #iterate through the words in tf, which is the words of a given 10k document of a given date\n",
    "        for word in all_data[ticker]['10ks'][date]['tf']:\n",
    "            \n",
    "            #add one if it already contains the word, or add the this word to the dict if not\n",
    "            word_list_10k[word] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute idf value for the word in 10ks\n",
    "idf_10k = {}\n",
    "\n",
    "#iterate through all the words in word_list_10k\n",
    "for word in word_list_10k:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10k[word] = np.log(document_num_10k / (1 + word_list_10k[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#we already have the number of all 10k files\n",
    "# document_num_10k = document_num_10k\n",
    "\n",
    "#word pair list for 10k\n",
    "pair_list_10k = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10ks']:\n",
    "        \n",
    "        #there is no need for counting document_num_10k\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (2, 2))\n",
    "        \n",
    "        #fit it through vectorizer\n",
    "        fitted = vectorizer.fit_transform([all_data[ticker]['10ks'][date]['words']])\n",
    "        \n",
    "        #after vectorizer, we have feature name and feature count, feed them to a dataframe\n",
    "        df_temp = pd.DataFrame(index = vectorizer.get_feature_names(), data = np.squeeze(fitted.toarray()))\n",
    "#         #then we can add the data into the main data set all_data\n",
    "        all_data[ticker]['10ks'][date]['tf_pair'] = df_temp.to_dict()[0]\n",
    "#         print(all_data[ticker]['10ks'][date]['tf_pair'])\n",
    "        #print(all_data[ticker]['10ks'][date]['tf_pair'])\n",
    "        #iterate through the pairs in tf_pair, which is the words of a given 10k document of a given date\n",
    "        for pair in all_data[ticker]['10ks'][date]['tf_pair']:\n",
    "            \n",
    "            #add one if it already contains the pair, or add the this pair to the dict if not have\n",
    "            pair_list_10k[pair] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_list_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#compute idf value for the pair in 10ks\n",
    "idf_10k_pair = {}\n",
    "\n",
    "#iterate through all the pairs in pair_list_10k\n",
    "for pair in pair_list_10k:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10k_pair[pair] = np.log(document_num_10k / (1 + pair_list_10k[pair]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#we already have the number of all 10k files\n",
    "# document_num_10k = document_num_10k\n",
    "\n",
    "#word triple list for 10k\n",
    "triple_list_10k = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10ks']:\n",
    "        \n",
    "        #there is no need for counting document_num_10k\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (3, 3))\n",
    "        \n",
    "        #fit it through vectorizer\n",
    "        fitted = vectorizer.fit_transform([all_data[ticker]['10ks'][date]['words']])\n",
    "        \n",
    "        #after vectorizer, we have feature name and feature count, feed them to a dataframe\n",
    "        df_temp = pd.DataFrame(index = vectorizer.get_feature_names(), data = np.squeeze(fitted.toarray()))\n",
    "        \n",
    "        #then we can add the data into the main data set all_data\n",
    "        all_data[ticker]['10ks'][date]['tf_triple'] = df_temp.to_dict()[0]\n",
    "        \n",
    "        #iterate through the triples in tf_triple, which is the words of a given 10k document of a given date\n",
    "        for triple in all_data[ticker]['10ks'][date]['tf_triple']:\n",
    "            \n",
    "            #add one if it already contains the triple, or add the this triple to the dict if not have\n",
    "            triple_list_10k[triple] += 1\n",
    "            \n",
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#compute idf value for the triple in 10ks\n",
    "idf_10k_triple = {}\n",
    "\n",
    "#iterate through all the triples in triple_list_10k\n",
    "for triple in triple_list_10k:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10k_triple[triple] = np.log(document_num_10k / (1 + triple_list_10k[triple]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doing the same to 10qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also do the same to 10q files\n",
    "\n",
    "\"\"\"\n",
    "This part is to compute the tf values for the words in 10qs and collect the overall word list for computing idf in the next step\n",
    "\"\"\"\n",
    "#count the number of 10Q documents we have\n",
    "document_num_10q = 0\n",
    "\n",
    "#word list for 10q \n",
    "word_list_10q = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10qs']:\n",
    "        \n",
    "        #we have document for a given date, so add 1 for document_num_10q\n",
    "        document_num_10q += 1\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        all_data[ticker]['10qs'][date]['tf'] = Counter(all_data[ticker]['10qs'][date]['words'].split())\n",
    "        \n",
    "        #iterate through the words in tf, which is the words of a given 10q document of a given date\n",
    "        for word in all_data[ticker]['10qs'][date]['tf']:\n",
    "            \n",
    "            #add one if it already contains the word, or add the this word to the dict if not\n",
    "            word_list_10q[word] += 1\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute idf value for the word in 10qs\n",
    "idf_10q = {}\n",
    "\n",
    "#iterate through all the words in word_list_10q\n",
    "for word in word_list_10q:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10q[word] = np.log(document_num_10q / (1 + word_list_10q[word]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#we already have the number of all 10q files\n",
    "# document_num_10q = document_num_10q\n",
    "\n",
    "#word pair list for 10q\n",
    "pair_list_10q = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10qs']:\n",
    "        \n",
    "        #there is no need for counting document_num_10q\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (2, 2))\n",
    "        \n",
    "        #fit it through vectorizer\n",
    "        fitted = vectorizer.fit_transform([all_data[ticker]['10qs'][date]['words']])\n",
    "        \n",
    "        #after vectorizer, we have feature name and feature count, feed them to a dataframe\n",
    "        df_temp = pd.DataFrame(index = vectorizer.get_feature_names(), data = np.squeeze(fitted.toarray()))\n",
    "        \n",
    "        #then we can add the data into the main data set all_data\n",
    "        all_data[ticker]['10qs'][date]['tf_pair'] = df_temp.to_dict()[0]\n",
    "        \n",
    "        #iterate through the pairs in tf_pair, which is the words of a given 10q document of a given date\n",
    "        for pair in all_data[ticker]['10qs'][date]['tf_pair']:\n",
    "            \n",
    "            #add one if it already contains the pair, or add the this pair to the dict if not have\n",
    "            pair_list_10q[pair] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#compute idf value for the pair in 10qs\n",
    "idf_10q_pair = {}\n",
    "\n",
    "#iterate through all the pairs in pair_list_10q\n",
    "for pair in pair_list_10q:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10q_pair[pair] = np.log(document_num_10q / (1 + pair_list_10q[pair]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#we already have the number of all 10q files\n",
    "# document_num_10q = document_num_10q\n",
    "\n",
    "#word triple list for 10q\n",
    "triple_list_10q = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10qs']:\n",
    "        \n",
    "        #there is no need for counting document_num_10q\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        \"\"\"\n",
    "        The only adaptation from pair to triple is changing ngram_range, needing futher simplification of code\n",
    "        \"\"\"\n",
    "        vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (3, 3))\n",
    "        \n",
    "        #fit it through vectorizer\n",
    "        fitted = vectorizer.fit_transform([all_data[ticker]['10qs'][date]['words']])\n",
    "        \n",
    "        #after vectorizer, we have feature name and feature count, feed them to a dataframe\n",
    "        df_temp = pd.DataFrame(index = vectorizer.get_feature_names(), data = np.squeeze(fitted.toarray()))\n",
    "        \n",
    "        #then we can add the data into the main data set all_data\n",
    "        all_data[ticker]['10qs'][date]['tf_triple'] = df_temp.to_dict()[0]\n",
    "        \n",
    "        #iterate through the triples in tf_triple, which is the words of a given 10q document of a given date\n",
    "        for triple in all_data[ticker]['10qs'][date]['tf_triple']:\n",
    "            \n",
    "            #add one if it already contains the triple, or add the this triple to the dict if not have\n",
    "            triple_list_10q[triple] += 1\n",
    "            \n",
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#compute idf value for the triple in 10qs\n",
    "idf_10q_triple = {}\n",
    "\n",
    "#iterate through all the triples in triple_list_10q\n",
    "for triple in triple_list_10q:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10q_triple[triple] = np.log(document_num_10q / (1 + triple_list_10q[triple]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### have a look at the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is new in version 2, containing idfs for word pairs\n",
    "\"\"\"\n",
    "\n",
    "idf_10q_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is new in version 2, containing idfs for word pairs\n",
    "\"\"\"\n",
    "\n",
    "idf_10q_triple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the data for future use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this part is for storing the data for future use\n",
    "\"\"\"\n",
    "#delete word in all_data for storage\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10qs']:\n",
    "        \n",
    "        del all_data[ticker]['10qs'][date]['words']\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10ks']:\n",
    "        \n",
    "        del all_data[ticker]['10ks'][date]['words']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write all_data to a json file\n",
    "with open('all_data.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(all_data))\n",
    "\n",
    "#write idf_10k to a json file\n",
    "with open('idf_10k.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10k))\n",
    "\n",
    "#write idf_10q to a json file    \n",
    "with open('idf_10q.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10q))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is new in version 2, storing files for word pairs and word triples\n",
    "\"\"\"\n",
    "#write idf_10k_pair to a json file  \n",
    "with open('idf_10k_pair.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10k_pair))\n",
    "\n",
    "#write idf_10q_pair to a json file    \n",
    "with open('idf_10q_pair.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10q_pair))\n",
    "    \n",
    "#write idf_10k_triple to a json file  \n",
    "with open('idf_10k_triple.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10k_triple))\n",
    "\n",
    "#write idf_10q_triple to a json file    \n",
    "with open('idf_10q_triple.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10q_triple))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
