{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/luckywang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/luckywang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/luckywang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Importing libraries you need to install\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from string import punctuation\n",
    "\n",
    "# Import yfinance and pandas_datareader\n",
    "from pandas_datareader import data as pdr\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "\n",
    "# import yfinance as yf \n",
    "\n",
    "# Override function to store data we get\n",
    "# yf.pdr_override()\n",
    "\n",
    "# Import nltk for first step extracting words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Set up stop_words from nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words |= {'10-k', 'form', 'table', 'contents', 'united', 'states', 'securities', 'exchange', 'commission'}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "\"\"\"\n",
    "this is where different from version 1\n",
    "\"\"\"\n",
    "#import libraries for n-gram counting\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now we have all the txts stored in the file:\n",
    "'./data/10k/[cik]/rawtext/[cik]_[date]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we can make a dictionary to store all the data needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luckywang/Documents/Document/Course Material/Fall 2021/esg_nlp/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# read the ticker library of all the tikers into ticker_library\n",
    "ticker_library = pd.read_csv(os.path.join(\"data\", \"tickers.csv\"))\n",
    "\n",
    "# read the sp500 components into ticker_selected, 'name' is the company name and ticker is company's ticker\n",
    "ticker_selected = pd.read_csv(os.path.join(\"data\", \"SP500_component_stocks.csv\"), header = None)\n",
    "ticker_selected.columns = ['name','ticker']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001090872</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000006201</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001158449</td>\n",
       "      <td>AAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000320193</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001551152</td>\n",
       "      <td>ABBV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cik ticker\n",
       "0  0001090872      A\n",
       "1  0000006201    AAL\n",
       "2  0001158449    AAP\n",
       "3  0000320193   AAPL\n",
       "4  0001551152   ABBV"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a ticker_cik_df dataframe to store ticker and its cik number\n",
    "ticker_cik_df = pd.DataFrame()\n",
    "\n",
    "# store all the tickers in a ticker_list\n",
    "ticker_list = ticker_selected.ticker\n",
    "\n",
    "# build a list cik_list for cik\n",
    "cik_list = []\n",
    "\n",
    "for ticker in ticker_list:    \n",
    "    try:\n",
    "        # for a given ticker, find its cik number through th ticker library\n",
    "        cik_list.append(list(ticker_library[ticker_library.ticker == ticker].secfilings)[0][-10:])\n",
    "        \n",
    "    except:\n",
    "        # if could not find cik, give it a empty cik\n",
    "        cik_list.append('')\n",
    "\n",
    "# write cik_list and ticker_list to the dataframe ticker_cik_df\n",
    "ticker_cik_df['cik'] = cik_list\n",
    "ticker_cik_df['ticker'] = ticker_list\n",
    "\n",
    "# delete the tickers with empty cik number\n",
    "ticker_cik_df = ticker_cik_df[ticker_cik_df['cik'] != '']\n",
    "\n",
    "# display a sample of ticker_cik_df\n",
    "ticker_cik_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIK2TICKER = {row[\"cik\"]: row[\"ticker\"] for _, row in ticker_cik_df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "listtickers = ['AMZN','BBY','BKNG','MCD','EBAY','F','HD','TGT','WHR','JPM','SIVB','CFG','C','ALL','IVZ','ETFC','MET','PFG','CBOE',\n",
    "              'CTL','IPG','VIAC','NFLX','CHTR','FB','TWTR','NWSA','FOXA','AMD','INTC','AAPL','LRCX','MSFT','NLOK','CTSH','ADS',\n",
    "              'WU','PAYC','ABT','CVS','PFE','JNJ','BIIB','INCY','HSIC','WAT','ALGN','EW']\n",
    "\n",
    "ticker_cik_sample = pd.DataFrame()\n",
    "\n",
    "for ticker in listtickers:\n",
    "    ticker_cik_sample = ticker_cik_sample.append(ticker_cik_df[ticker_cik_df['ticker'] == ticker])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list = ticker_cik_sample[\"cik\"].values\n",
    "ticker_list = ticker_cik_sample[\"ticker\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a sample to analysis the overall program\n",
    "# use a sample to analysis the overall program\n",
    "# sample1 = ticker_cik_df.iloc[0:50]\n",
    "# sample2 = ticker_cik_df.iloc[50:100]\n",
    "# sample3 = ticker_cik_df.iloc[100:150]\n",
    "# sample4 = ticker_cik_df.iloc[150:200]\n",
    "# sample5 = ticker_cik_df.iloc[200:250]\n",
    "# sample6 = ticker_cik_df.iloc[250:300]\n",
    "# sample7 = ticker_cik_df.iloc[300:350]\n",
    "# sample8 = ticker_cik_df.iloc[350:400]\n",
    "# sample9 = ticker_cik_df.iloc[400:450]\n",
    "# sample10 = ticker_cik_df.iloc[450:500]\n",
    "# ticker_cik_sample = ['0000049826','0000816284']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store data using dictionary\n",
    "all_data = {}\n",
    "\n",
    "# set the key of dictionary as ticker\n",
    "for cik, ticker in zip(cik_list, ticker_list):\n",
    "    \n",
    "    # set the value of tikcer as a dict\n",
    "    all_data[ticker] = {}\n",
    "\n",
    "    # set the dict data[ticker] \n",
    "    all_data[ticker]['cik'] = cik\n",
    "    all_data[ticker]['10ks'] = {}\n",
    "    all_data[ticker]['10qs'] = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cik': '0000049826', '10ks': {}, '10qs': {}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['ITW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_10k = './data/10k/'\n",
    "dir_10q = './data/10q/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "print(len(cik_list))\n",
    "print(len(ticker_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to remove punctuations if a given word ended with a punctuation\n",
    "def remove_punct(string):\n",
    "    return re.sub(r\"[{}]+\".format(punctuation), \"\", string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for filtering words\n",
    "def filter_words(string):\n",
    "    return bool(re.match(r'^[a-z\\']+$', string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_cik_texts(cik, filetype):\n",
    "    \"\"\"\n",
    "    Collect all the texts related to given `cik` with given filetype and \n",
    "    return a single string\n",
    "    \"\"\"\n",
    "    \n",
    "    assert filetype in (\"10k\", \"10q\")\n",
    "    \n",
    "    cik_dir = os.path.join(\"data\", filetype, cik)\n",
    "    rawtext_dir = os.path.join(cik_dir, \"rawtext\")\n",
    "    # goes into the directory to find the path for txtfiles\n",
    "    try:\n",
    "        all_files = os.listdir(rawtext_dir)\n",
    "    except:\n",
    "        print(\"No such dir\")\n",
    "    \n",
    "    texts = \"\"\n",
    "    for file in all_files:\n",
    "        with open(os.path.join(\"data\", filetype, cik, \"rawtext\", file), encoding = \"utf8\") as f:\n",
    "            string_temp = f.read().lower()\n",
    "            texts += string_temp\n",
    "    \n",
    "    texts = remove_punct(texts)\n",
    "    \n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts2counter(texts):\n",
    "    tokens = [token for token in nltk.word_tokenize(texts) if token not in stop_words and not token.isdigit()]\n",
    "    tokens = list(filter(filter_words, tokens))\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    pkl_path = os.path.join(cik_dir, \"pickle\")\n",
    "    if not os.path.isdir(pkl_path):\n",
    "        os.mkdir(pkl_path)\n",
    "    \n",
    "    with open(os.path.join(pkl_path, 'token_counter.pkl'), 'wb') as f:\n",
    "        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "        pickle.dump(counter, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return counter\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [00:37<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# './data/10k/[cik]/rawtext/[cik]_[date]'\n",
    "docs = []\n",
    "tickers = []\n",
    "\n",
    "for cik in tqdm(cik_list):\n",
    "    tickers.append(CIK2TICKER[cik])\n",
    "    texts = \"\"\n",
    "    for filetype in [\"10k\", \"10q\"]:\n",
    "        texts += aggregate_cik_texts(cik, filetype)\n",
    "        # counter = texts2counter(texts)\n",
    "    docs.append(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luckywang/Documents/Document/Course Material/Fall 2021/esg_nlp/venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['10'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(max_df=0.8, stop_words=stop_words, max_features=10000)\n",
    "word_count_vector = cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results = {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]] = score_vals[idx]\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "# tf_idf_vector = tfidf_transformer.transform(word_count_vector)\n",
    "# tf_idf_vector = tfidf_transformer.transform(cv.transform([docs[3]]))\n",
    "# tf_idf_vector = tfidf_transformer.transform(cv.transform(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = []\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    tf_idf_vector = tfidf_transformer.transform(cv.transform([docs[i]]))\n",
    "    # you only needs to do this once, this is a mapping of index to \n",
    "    feature_names = cv.get_feature_names()\n",
    "\n",
    "    sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "    # extract only the top n; n here is 10\n",
    "    keyword = extract_topn_from_vector(feature_names, sorted_items, 30)\n",
    "    \n",
    "    keywords.append(keyword)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_top_k = defaultdict(list)\n",
    "dict_top_k[\"tickers\"] = tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keyword in keywords:\n",
    "    for i, word in enumerate(keyword.keys()):\n",
    "        dict_top_k[\"word_{}\".format(i)].append(word)\n",
    "        dict_top_k[\"tfidf_{}\".format(i)].append(keyword[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_k_word = pd.DataFrame(dict_top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tickers</th>\n",
       "      <th>word_0</th>\n",
       "      <th>tfidf_0</th>\n",
       "      <th>word_1</th>\n",
       "      <th>tfidf_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>tfidf_2</th>\n",
       "      <th>word_3</th>\n",
       "      <th>tfidf_3</th>\n",
       "      <th>word_4</th>\n",
       "      <th>...</th>\n",
       "      <th>word_25</th>\n",
       "      <th>tfidf_25</th>\n",
       "      <th>word_26</th>\n",
       "      <th>tfidf_26</th>\n",
       "      <th>word_27</th>\n",
       "      <th>tfidf_27</th>\n",
       "      <th>word_28</th>\n",
       "      <th>tfidf_28</th>\n",
       "      <th>word_29</th>\n",
       "      <th>tfidf_29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>amazoncom</td>\n",
       "      <td>0.438</td>\n",
       "      <td>peacs</td>\n",
       "      <td>0.373</td>\n",
       "      <td>fulfillment</td>\n",
       "      <td>0.368</td>\n",
       "      <td>amazon</td>\n",
       "      <td>0.245</td>\n",
       "      <td>shipping</td>\n",
       "      <td>...</td>\n",
       "      <td>euros</td>\n",
       "      <td>0.059</td>\n",
       "      <td>card</td>\n",
       "      <td>0.059</td>\n",
       "      <td>wwwamazonca</td>\n",
       "      <td>0.056</td>\n",
       "      <td>wwwamazoncom</td>\n",
       "      <td>0.055</td>\n",
       "      <td>unearned</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BBY</td>\n",
       "      <td>stores</td>\n",
       "      <td>0.641</td>\n",
       "      <td>store</td>\n",
       "      <td>0.378</td>\n",
       "      <td>sga</td>\n",
       "      <td>0.283</td>\n",
       "      <td>musicland</td>\n",
       "      <td>0.176</td>\n",
       "      <td>merchandise</td>\n",
       "      <td>...</td>\n",
       "      <td>largeformat</td>\n",
       "      <td>0.062</td>\n",
       "      <td>speakeasy</td>\n",
       "      <td>0.059</td>\n",
       "      <td>notebook</td>\n",
       "      <td>0.051</td>\n",
       "      <td>canadian</td>\n",
       "      <td>0.050</td>\n",
       "      <td>auctionrate</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BKNG</td>\n",
       "      <td>hotel</td>\n",
       "      <td>0.420</td>\n",
       "      <td>pricelinecom</td>\n",
       "      <td>0.404</td>\n",
       "      <td>bookingcom</td>\n",
       "      <td>0.347</td>\n",
       "      <td>reservations</td>\n",
       "      <td>0.273</td>\n",
       "      <td>airline</td>\n",
       "      <td>...</td>\n",
       "      <td>search</td>\n",
       "      <td>0.068</td>\n",
       "      <td>airlines</td>\n",
       "      <td>0.067</td>\n",
       "      <td>otcs</td>\n",
       "      <td>0.067</td>\n",
       "      <td>expedia</td>\n",
       "      <td>0.066</td>\n",
       "      <td>braddock</td>\n",
       "      <td>0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MCD</td>\n",
       "      <td>restaurants</td>\n",
       "      <td>0.478</td>\n",
       "      <td>mcdonald</td>\n",
       "      <td>0.467</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>0.383</td>\n",
       "      <td>companyoperated</td>\n",
       "      <td>0.348</td>\n",
       "      <td>mcdonalds</td>\n",
       "      <td>...</td>\n",
       "      <td>conventional</td>\n",
       "      <td>0.033</td>\n",
       "      <td>franchisee</td>\n",
       "      <td>0.032</td>\n",
       "      <td>japan</td>\n",
       "      <td>0.032</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>0.032</td>\n",
       "      <td>chicken</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EBAY</td>\n",
       "      <td>paypal</td>\n",
       "      <td>0.819</td>\n",
       "      <td>ebay</td>\n",
       "      <td>0.423</td>\n",
       "      <td>skype</td>\n",
       "      <td>0.144</td>\n",
       "      <td>marketplaces</td>\n",
       "      <td>0.106</td>\n",
       "      <td>paypals</td>\n",
       "      <td>...</td>\n",
       "      <td>halfcom</td>\n",
       "      <td>0.035</td>\n",
       "      <td>tpv</td>\n",
       "      <td>0.034</td>\n",
       "      <td>ebaycom</td>\n",
       "      <td>0.033</td>\n",
       "      <td>search</td>\n",
       "      <td>0.031</td>\n",
       "      <td>auction</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F</td>\n",
       "      <td>ford</td>\n",
       "      <td>0.888</td>\n",
       "      <td>automotive</td>\n",
       "      <td>0.265</td>\n",
       "      <td>motor</td>\n",
       "      <td>0.154</td>\n",
       "      <td>vehicles</td>\n",
       "      <td>0.133</td>\n",
       "      <td>incomeloss</td>\n",
       "      <td>...</td>\n",
       "      <td>jaguar</td>\n",
       "      <td>0.033</td>\n",
       "      <td>nonconsumer</td>\n",
       "      <td>0.032</td>\n",
       "      <td>fcar</td>\n",
       "      <td>0.031</td>\n",
       "      <td>assetbacked</td>\n",
       "      <td>0.031</td>\n",
       "      <td>commodity</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HD</td>\n",
       "      <td>depot</td>\n",
       "      <td>0.720</td>\n",
       "      <td>stores</td>\n",
       "      <td>0.440</td>\n",
       "      <td>store</td>\n",
       "      <td>0.298</td>\n",
       "      <td>hd</td>\n",
       "      <td>0.167</td>\n",
       "      <td>merchandise</td>\n",
       "      <td>...</td>\n",
       "      <td>chain</td>\n",
       "      <td>0.045</td>\n",
       "      <td>blake</td>\n",
       "      <td>0.042</td>\n",
       "      <td>shrink</td>\n",
       "      <td>0.041</td>\n",
       "      <td>lighting</td>\n",
       "      <td>0.041</td>\n",
       "      <td>gift</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TGT</td>\n",
       "      <td>card</td>\n",
       "      <td>0.582</td>\n",
       "      <td>stores</td>\n",
       "      <td>0.355</td>\n",
       "      <td>guests</td>\n",
       "      <td>0.254</td>\n",
       "      <td>comparablestore</td>\n",
       "      <td>0.248</td>\n",
       "      <td>merchandise</td>\n",
       "      <td>...</td>\n",
       "      <td>minnesota</td>\n",
       "      <td>0.059</td>\n",
       "      <td>gift</td>\n",
       "      <td>0.058</td>\n",
       "      <td>douglas</td>\n",
       "      <td>0.057</td>\n",
       "      <td>cvs</td>\n",
       "      <td>0.055</td>\n",
       "      <td>3a</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WHR</td>\n",
       "      <td>whirlpool</td>\n",
       "      <td>0.807</td>\n",
       "      <td>befiex</td>\n",
       "      <td>0.241</td>\n",
       "      <td>brazilian</td>\n",
       "      <td>0.193</td>\n",
       "      <td>maytag</td>\n",
       "      <td>0.186</td>\n",
       "      <td>indesit</td>\n",
       "      <td>...</td>\n",
       "      <td>forwardsoptions</td>\n",
       "      <td>0.046</td>\n",
       "      <td>oilrelated</td>\n",
       "      <td>0.044</td>\n",
       "      <td>monetized</td>\n",
       "      <td>0.041</td>\n",
       "      <td>hotpoint</td>\n",
       "      <td>0.041</td>\n",
       "      <td>compressors</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>JPM</td>\n",
       "      <td>jpmorgan</td>\n",
       "      <td>0.539</td>\n",
       "      <td>lendingrelated</td>\n",
       "      <td>0.276</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>0.246</td>\n",
       "      <td>card</td>\n",
       "      <td>0.227</td>\n",
       "      <td>pages</td>\n",
       "      <td>...</td>\n",
       "      <td>msr</td>\n",
       "      <td>0.080</td>\n",
       "      <td>afs</td>\n",
       "      <td>0.079</td>\n",
       "      <td>tier</td>\n",
       "      <td>0.078</td>\n",
       "      <td>rfs</td>\n",
       "      <td>0.077</td>\n",
       "      <td>lending</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SIVB</td>\n",
       "      <td>svb</td>\n",
       "      <td>0.885</td>\n",
       "      <td>noninterest</td>\n",
       "      <td>0.191</td>\n",
       "      <td>clients</td>\n",
       "      <td>0.175</td>\n",
       "      <td>client</td>\n",
       "      <td>0.121</td>\n",
       "      <td>warrant</td>\n",
       "      <td>...</td>\n",
       "      <td>riskweighted</td>\n",
       "      <td>0.039</td>\n",
       "      <td>standby</td>\n",
       "      <td>0.039</td>\n",
       "      <td>reflective</td>\n",
       "      <td>0.038</td>\n",
       "      <td>volcker</td>\n",
       "      <td>0.036</td>\n",
       "      <td>debentures</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CFG</td>\n",
       "      <td>incmanagement</td>\n",
       "      <td>0.326</td>\n",
       "      <td>basel</td>\n",
       "      <td>0.269</td>\n",
       "      <td>cbna</td>\n",
       "      <td>0.257</td>\n",
       "      <td>frb</td>\n",
       "      <td>0.249</td>\n",
       "      <td>rbs</td>\n",
       "      <td>...</td>\n",
       "      <td>noncore</td>\n",
       "      <td>0.079</td>\n",
       "      <td>msrs</td>\n",
       "      <td>0.079</td>\n",
       "      <td>lending</td>\n",
       "      <td>0.078</td>\n",
       "      <td>lcr</td>\n",
       "      <td>0.077</td>\n",
       "      <td>servicing</td>\n",
       "      <td>0.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>C</td>\n",
       "      <td>citi</td>\n",
       "      <td>0.846</td>\n",
       "      <td>citigroup</td>\n",
       "      <td>0.349</td>\n",
       "      <td>citis</td>\n",
       "      <td>0.145</td>\n",
       "      <td>citigroups</td>\n",
       "      <td>0.110</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>...</td>\n",
       "      <td>vies</td>\n",
       "      <td>0.041</td>\n",
       "      <td>fx</td>\n",
       "      <td>0.041</td>\n",
       "      <td>securitization</td>\n",
       "      <td>0.040</td>\n",
       "      <td>residential</td>\n",
       "      <td>0.040</td>\n",
       "      <td>htm</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ALL</td>\n",
       "      <td>allstate</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0000899051</td>\n",
       "      <td>0.384</td>\n",
       "      <td>propertyliability</td>\n",
       "      <td>0.225</td>\n",
       "      <td>reinsurance</td>\n",
       "      <td>0.225</td>\n",
       "      <td>contractholder</td>\n",
       "      <td>...</td>\n",
       "      <td>ibnr</td>\n",
       "      <td>0.038</td>\n",
       "      <td>pif</td>\n",
       "      <td>0.037</td>\n",
       "      <td>mcca</td>\n",
       "      <td>0.035</td>\n",
       "      <td>spreads</td>\n",
       "      <td>0.035</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>IVZ</td>\n",
       "      <td>aum</td>\n",
       "      <td>0.661</td>\n",
       "      <td>invesco</td>\n",
       "      <td>0.621</td>\n",
       "      <td>clos</td>\n",
       "      <td>0.279</td>\n",
       "      <td>cip</td>\n",
       "      <td>0.225</td>\n",
       "      <td>clo</td>\n",
       "      <td>...</td>\n",
       "      <td>proportional</td>\n",
       "      <td>0.023</td>\n",
       "      <td>subgroup</td>\n",
       "      <td>0.022</td>\n",
       "      <td>sterling</td>\n",
       "      <td>0.022</td>\n",
       "      <td>amvescap</td>\n",
       "      <td>0.021</td>\n",
       "      <td>unconsolidated</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ETFC</td>\n",
       "      <td>etrade</td>\n",
       "      <td>0.873</td>\n",
       "      <td>fourfamily</td>\n",
       "      <td>0.166</td>\n",
       "      <td>brokerage</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0001015780</td>\n",
       "      <td>0.141</td>\n",
       "      <td>ajaxo</td>\n",
       "      <td>...</td>\n",
       "      <td>lending</td>\n",
       "      <td>0.041</td>\n",
       "      <td>contentsetrade</td>\n",
       "      <td>0.039</td>\n",
       "      <td>ltvcltv</td>\n",
       "      <td>0.036</td>\n",
       "      <td>etbh</td>\n",
       "      <td>0.036</td>\n",
       "      <td>nonaccrual</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MET</td>\n",
       "      <td>metlife</td>\n",
       "      <td>0.794</td>\n",
       "      <td>policyholder</td>\n",
       "      <td>0.322</td>\n",
       "      <td>reinsurance</td>\n",
       "      <td>0.183</td>\n",
       "      <td>mlic</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0001099219</td>\n",
       "      <td>...</td>\n",
       "      <td>rmbs</td>\n",
       "      <td>0.039</td>\n",
       "      <td>agricultural</td>\n",
       "      <td>0.036</td>\n",
       "      <td>surplus</td>\n",
       "      <td>0.034</td>\n",
       "      <td>alico</td>\n",
       "      <td>0.033</td>\n",
       "      <td>afs</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PFG</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>0.530</td>\n",
       "      <td>accumulation</td>\n",
       "      <td>0.205</td>\n",
       "      <td>pfg</td>\n",
       "      <td>0.199</td>\n",
       "      <td>residential</td>\n",
       "      <td>0.199</td>\n",
       "      <td>annuities</td>\n",
       "      <td>...</td>\n",
       "      <td>actuarial</td>\n",
       "      <td>0.083</td>\n",
       "      <td>restructured</td>\n",
       "      <td>0.079</td>\n",
       "      <td>indemnifications</td>\n",
       "      <td>0.077</td>\n",
       "      <td>cmbs</td>\n",
       "      <td>0.076</td>\n",
       "      <td>contentsprincipal</td>\n",
       "      <td>0.073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CBOE</td>\n",
       "      <td>cboe</td>\n",
       "      <td>0.895</td>\n",
       "      <td>bats</td>\n",
       "      <td>0.236</td>\n",
       "      <td>c2</td>\n",
       "      <td>0.120</td>\n",
       "      <td>futures</td>\n",
       "      <td>0.112</td>\n",
       "      <td>clearing</td>\n",
       "      <td>...</td>\n",
       "      <td>marketmaker</td>\n",
       "      <td>0.034</td>\n",
       "      <td>exchangetraded</td>\n",
       "      <td>0.034</td>\n",
       "      <td>multiplylisted</td>\n",
       "      <td>0.033</td>\n",
       "      <td>unrestricted</td>\n",
       "      <td>0.032</td>\n",
       "      <td>outcry</td>\n",
       "      <td>0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CTL</td>\n",
       "      <td>centurylink</td>\n",
       "      <td>0.713</td>\n",
       "      <td>qwest</td>\n",
       "      <td>0.320</td>\n",
       "      <td>00107784</td>\n",
       "      <td>0.309</td>\n",
       "      <td>embarq</td>\n",
       "      <td>0.242</td>\n",
       "      <td>centurytel</td>\n",
       "      <td>...</td>\n",
       "      <td>hosting</td>\n",
       "      <td>0.045</td>\n",
       "      <td>verizon</td>\n",
       "      <td>0.043</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.043</td>\n",
       "      <td>carriers</td>\n",
       "      <td>0.042</td>\n",
       "      <td>ilecs</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>IPG</td>\n",
       "      <td>interpublic</td>\n",
       "      <td>0.712</td>\n",
       "      <td>clients</td>\n",
       "      <td>0.266</td>\n",
       "      <td>cmg</td>\n",
       "      <td>0.225</td>\n",
       "      <td>ian</td>\n",
       "      <td>0.187</td>\n",
       "      <td>organic</td>\n",
       "      <td>...</td>\n",
       "      <td>acxiom</td>\n",
       "      <td>0.058</td>\n",
       "      <td>reorganizationrelated</td>\n",
       "      <td>0.055</td>\n",
       "      <td>uncommitted</td>\n",
       "      <td>0.052</td>\n",
       "      <td>philippe</td>\n",
       "      <td>0.051</td>\n",
       "      <td>interpublics</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NFLX</td>\n",
       "      <td>streaming</td>\n",
       "      <td>0.480</td>\n",
       "      <td>dvd</td>\n",
       "      <td>0.412</td>\n",
       "      <td>subscribers</td>\n",
       "      <td>0.392</td>\n",
       "      <td>netflix</td>\n",
       "      <td>0.265</td>\n",
       "      <td>dvds</td>\n",
       "      <td>...</td>\n",
       "      <td>s1a</td>\n",
       "      <td>0.042</td>\n",
       "      <td>tv</td>\n",
       "      <td>0.042</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>0.041</td>\n",
       "      <td>dvdbymail</td>\n",
       "      <td>0.041</td>\n",
       "      <td>hastings</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>CHTR</td>\n",
       "      <td>cable</td>\n",
       "      <td>0.456</td>\n",
       "      <td>cco</td>\n",
       "      <td>0.390</td>\n",
       "      <td>holdco</td>\n",
       "      <td>0.322</td>\n",
       "      <td>cch</td>\n",
       "      <td>0.283</td>\n",
       "      <td>twc</td>\n",
       "      <td>...</td>\n",
       "      <td>subsidiariesnotes</td>\n",
       "      <td>0.068</td>\n",
       "      <td>franchising</td>\n",
       "      <td>0.065</td>\n",
       "      <td>indentures</td>\n",
       "      <td>0.065</td>\n",
       "      <td>viii</td>\n",
       "      <td>0.061</td>\n",
       "      <td>comcast</td>\n",
       "      <td>0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>FB</td>\n",
       "      <td>facebook</td>\n",
       "      <td>0.580</td>\n",
       "      <td>user</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0001326801</td>\n",
       "      <td>0.283</td>\n",
       "      <td>maus</td>\n",
       "      <td>0.256</td>\n",
       "      <td>ads</td>\n",
       "      <td>...</td>\n",
       "      <td>undesirable</td>\n",
       "      <td>0.039</td>\n",
       "      <td>geography</td>\n",
       "      <td>0.037</td>\n",
       "      <td>dau</td>\n",
       "      <td>0.035</td>\n",
       "      <td>patent</td>\n",
       "      <td>0.030</td>\n",
       "      <td>monetize</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>TWTR</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0.602</td>\n",
       "      <td>advertisers</td>\n",
       "      <td>0.402</td>\n",
       "      <td>user</td>\n",
       "      <td>0.303</td>\n",
       "      <td>mdau</td>\n",
       "      <td>0.207</td>\n",
       "      <td>tweets</td>\n",
       "      <td>...</td>\n",
       "      <td>search</td>\n",
       "      <td>0.051</td>\n",
       "      <td>video</td>\n",
       "      <td>0.049</td>\n",
       "      <td>privatelyheld</td>\n",
       "      <td>0.047</td>\n",
       "      <td>pre2013</td>\n",
       "      <td>0.044</td>\n",
       "      <td>333191552</td>\n",
       "      <td>0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NWSA</td>\n",
       "      <td>fox</td>\n",
       "      <td>0.415</td>\n",
       "      <td>news</td>\n",
       "      <td>0.375</td>\n",
       "      <td>rea</td>\n",
       "      <td>0.337</td>\n",
       "      <td>foxtel</td>\n",
       "      <td>0.325</td>\n",
       "      <td>21st</td>\n",
       "      <td>...</td>\n",
       "      <td>jones</td>\n",
       "      <td>0.054</td>\n",
       "      <td>amplify</td>\n",
       "      <td>0.053</td>\n",
       "      <td>paytv</td>\n",
       "      <td>0.046</td>\n",
       "      <td>betterworse</td>\n",
       "      <td>0.043</td>\n",
       "      <td>newsprint</td>\n",
       "      <td>0.043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>FOXA</td>\n",
       "      <td>0001754301</td>\n",
       "      <td>0.645</td>\n",
       "      <td>fox</td>\n",
       "      <td>0.512</td>\n",
       "      <td>21cf</td>\n",
       "      <td>0.423</td>\n",
       "      <td>sports</td>\n",
       "      <td>0.139</td>\n",
       "      <td>programming</td>\n",
       "      <td>...</td>\n",
       "      <td>20201231</td>\n",
       "      <td>0.039</td>\n",
       "      <td>mvpds</td>\n",
       "      <td>0.037</td>\n",
       "      <td>20200701</td>\n",
       "      <td>0.037</td>\n",
       "      <td>heading</td>\n",
       "      <td>0.035</td>\n",
       "      <td>lot</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>AMD</td>\n",
       "      <td>amd</td>\n",
       "      <td>0.667</td>\n",
       "      <td>gf</td>\n",
       "      <td>0.355</td>\n",
       "      <td>fab</td>\n",
       "      <td>0.298</td>\n",
       "      <td>spansion</td>\n",
       "      <td>0.275</td>\n",
       "      <td>graphics</td>\n",
       "      <td>...</td>\n",
       "      <td>foundry</td>\n",
       "      <td>0.053</td>\n",
       "      <td>atic</td>\n",
       "      <td>0.052</td>\n",
       "      <td>atmp</td>\n",
       "      <td>0.052</td>\n",
       "      <td>wafers</td>\n",
       "      <td>0.051</td>\n",
       "      <td>processors</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>INTC</td>\n",
       "      <td>intel</td>\n",
       "      <td>0.864</td>\n",
       "      <td>microprocessors</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0000050863</td>\n",
       "      <td>0.141</td>\n",
       "      <td>memory</td>\n",
       "      <td>0.140</td>\n",
       "      <td>imft</td>\n",
       "      <td>...</td>\n",
       "      <td>ec</td>\n",
       "      <td>0.049</td>\n",
       "      <td>iotg</td>\n",
       "      <td>0.048</td>\n",
       "      <td>numonyx</td>\n",
       "      <td>0.047</td>\n",
       "      <td>corporationnotes</td>\n",
       "      <td>0.043</td>\n",
       "      <td>desktop</td>\n",
       "      <td>0.043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>apple</td>\n",
       "      <td>0.378</td>\n",
       "      <td>ipod</td>\n",
       "      <td>0.316</td>\n",
       "      <td>mac</td>\n",
       "      <td>0.314</td>\n",
       "      <td>iphone</td>\n",
       "      <td>0.262</td>\n",
       "      <td>macintosh</td>\n",
       "      <td>...</td>\n",
       "      <td>americas</td>\n",
       "      <td>0.065</td>\n",
       "      <td>education</td>\n",
       "      <td>0.064</td>\n",
       "      <td>applebranded</td>\n",
       "      <td>0.062</td>\n",
       "      <td>patent</td>\n",
       "      <td>0.060</td>\n",
       "      <td>microprocessors</td>\n",
       "      <td>0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>LRCX</td>\n",
       "      <td>lam</td>\n",
       "      <td>0.592</td>\n",
       "      <td>semiconductor</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0000707549</td>\n",
       "      <td>0.396</td>\n",
       "      <td>etch</td>\n",
       "      <td>0.215</td>\n",
       "      <td>wafer</td>\n",
       "      <td>...</td>\n",
       "      <td>singlewafer</td>\n",
       "      <td>0.049</td>\n",
       "      <td>20180923</td>\n",
       "      <td>0.049</td>\n",
       "      <td>bullen</td>\n",
       "      <td>0.048</td>\n",
       "      <td>klatencor</td>\n",
       "      <td>0.048</td>\n",
       "      <td>espp</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>microsoft</td>\n",
       "      <td>0.559</td>\n",
       "      <td>windows</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0000789019</td>\n",
       "      <td>0.315</td>\n",
       "      <td>xbox</td>\n",
       "      <td>0.267</td>\n",
       "      <td>server</td>\n",
       "      <td>...</td>\n",
       "      <td>phone</td>\n",
       "      <td>0.051</td>\n",
       "      <td>subscriptions</td>\n",
       "      <td>0.050</td>\n",
       "      <td>intelligent</td>\n",
       "      <td>0.049</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>0.049</td>\n",
       "      <td>surface</td>\n",
       "      <td>0.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>CTSH</td>\n",
       "      <td>cognizant</td>\n",
       "      <td>0.634</td>\n",
       "      <td>indian</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0001058290</td>\n",
       "      <td>0.295</td>\n",
       "      <td>clients</td>\n",
       "      <td>0.270</td>\n",
       "      <td>rupee</td>\n",
       "      <td>...</td>\n",
       "      <td>souza</td>\n",
       "      <td>0.055</td>\n",
       "      <td>visas</td>\n",
       "      <td>0.055</td>\n",
       "      <td>itd</td>\n",
       "      <td>0.055</td>\n",
       "      <td>rupees</td>\n",
       "      <td>0.054</td>\n",
       "      <td>visa</td>\n",
       "      <td>0.053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ADS</td>\n",
       "      <td>card</td>\n",
       "      <td>0.524</td>\n",
       "      <td>miles</td>\n",
       "      <td>0.342</td>\n",
       "      <td>wfn</td>\n",
       "      <td>0.285</td>\n",
       "      <td>air</td>\n",
       "      <td>0.221</td>\n",
       "      <td>reward</td>\n",
       "      <td>...</td>\n",
       "      <td>canadian</td>\n",
       "      <td>0.055</td>\n",
       "      <td>33360418</td>\n",
       "      <td>0.054</td>\n",
       "      <td>mile</td>\n",
       "      <td>0.054</td>\n",
       "      <td>merchant</td>\n",
       "      <td>0.051</td>\n",
       "      <td>servicing</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>WU</td>\n",
       "      <td>consumertoconsumer</td>\n",
       "      <td>0.595</td>\n",
       "      <td>companynotes</td>\n",
       "      <td>0.343</td>\n",
       "      <td>spinoff</td>\n",
       "      <td>0.240</td>\n",
       "      <td>consumertobusiness</td>\n",
       "      <td>0.212</td>\n",
       "      <td>speedpay</td>\n",
       "      <td>...</td>\n",
       "      <td>fexco</td>\n",
       "      <td>0.064</td>\n",
       "      <td>gainslosses</td>\n",
       "      <td>0.061</td>\n",
       "      <td>mexico</td>\n",
       "      <td>0.060</td>\n",
       "      <td>crossborder</td>\n",
       "      <td>0.059</td>\n",
       "      <td>fa</td>\n",
       "      <td>0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ABT</td>\n",
       "      <td>abbott</td>\n",
       "      <td>0.932</td>\n",
       "      <td>abbotts</td>\n",
       "      <td>0.221</td>\n",
       "      <td>laboratories</td>\n",
       "      <td>0.181</td>\n",
       "      <td>pharmaceutical</td>\n",
       "      <td>0.097</td>\n",
       "      <td>tap</td>\n",
       "      <td>...</td>\n",
       "      <td>manufacturing</td>\n",
       "      <td>0.018</td>\n",
       "      <td>generics</td>\n",
       "      <td>0.018</td>\n",
       "      <td>humira</td>\n",
       "      <td>0.017</td>\n",
       "      <td>drug</td>\n",
       "      <td>0.017</td>\n",
       "      <td>xience</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>CVS</td>\n",
       "      <td>pharmacy</td>\n",
       "      <td>0.616</td>\n",
       "      <td>cvs</td>\n",
       "      <td>0.374</td>\n",
       "      <td>caremark</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0000064803</td>\n",
       "      <td>0.263</td>\n",
       "      <td>pharmacies</td>\n",
       "      <td>...</td>\n",
       "      <td>clients</td>\n",
       "      <td>0.049</td>\n",
       "      <td>cms</td>\n",
       "      <td>0.044</td>\n",
       "      <td>drugstores</td>\n",
       "      <td>0.043</td>\n",
       "      <td>minuteclinic</td>\n",
       "      <td>0.036</td>\n",
       "      <td>payors</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>PFE</td>\n",
       "      <td>pfizer</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0000078003</td>\n",
       "      <td>0.438</td>\n",
       "      <td>generic</td>\n",
       "      <td>0.165</td>\n",
       "      <td>fda</td>\n",
       "      <td>0.153</td>\n",
       "      <td>lipitor</td>\n",
       "      <td>...</td>\n",
       "      <td>pfebiopharmasegmentmember</td>\n",
       "      <td>0.067</td>\n",
       "      <td>warnerlambert</td>\n",
       "      <td>0.066</td>\n",
       "      <td>rd</td>\n",
       "      <td>0.064</td>\n",
       "      <td>patents</td>\n",
       "      <td>0.059</td>\n",
       "      <td>drugs</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>JNJ</td>\n",
       "      <td>johnson</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0000200406</td>\n",
       "      <td>0.399</td>\n",
       "      <td>janssen</td>\n",
       "      <td>0.289</td>\n",
       "      <td>patent</td>\n",
       "      <td>0.187</td>\n",
       "      <td>pharmaceutical</td>\n",
       "      <td>...</td>\n",
       "      <td>infringement</td>\n",
       "      <td>0.064</td>\n",
       "      <td>teva</td>\n",
       "      <td>0.061</td>\n",
       "      <td>usgaapnonusmember</td>\n",
       "      <td>0.059</td>\n",
       "      <td>biosimilar</td>\n",
       "      <td>0.058</td>\n",
       "      <td>asrtm</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>BIIB</td>\n",
       "      <td>tysabri</td>\n",
       "      <td>0.412</td>\n",
       "      <td>biogen</td>\n",
       "      <td>0.395</td>\n",
       "      <td>rituxan</td>\n",
       "      <td>0.372</td>\n",
       "      <td>idec</td>\n",
       "      <td>0.305</td>\n",
       "      <td>avonex</td>\n",
       "      <td>...</td>\n",
       "      <td>milestone</td>\n",
       "      <td>0.060</td>\n",
       "      <td>drug</td>\n",
       "      <td>0.058</td>\n",
       "      <td>trials</td>\n",
       "      <td>0.057</td>\n",
       "      <td>neurimmune</td>\n",
       "      <td>0.056</td>\n",
       "      <td>royalty</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>INCY</td>\n",
       "      <td>clinical</td>\n",
       "      <td>0.402</td>\n",
       "      <td>drug</td>\n",
       "      <td>0.392</td>\n",
       "      <td>jakafi</td>\n",
       "      <td>0.387</td>\n",
       "      <td>trials</td>\n",
       "      <td>0.202</td>\n",
       "      <td>collaborators</td>\n",
       "      <td>...</td>\n",
       "      <td>licensees</td>\n",
       "      <td>0.065</td>\n",
       "      <td>myelofibrosis</td>\n",
       "      <td>0.063</td>\n",
       "      <td>calithera</td>\n",
       "      <td>0.063</td>\n",
       "      <td>maxia</td>\n",
       "      <td>0.063</td>\n",
       "      <td>candidate</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>HSIC</td>\n",
       "      <td>schein</td>\n",
       "      <td>0.670</td>\n",
       "      <td>henry</td>\n",
       "      <td>0.381</td>\n",
       "      <td>dental</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0001000228</td>\n",
       "      <td>0.284</td>\n",
       "      <td>practitioners</td>\n",
       "      <td>...</td>\n",
       "      <td>bergman</td>\n",
       "      <td>0.044</td>\n",
       "      <td>patterson</td>\n",
       "      <td>0.043</td>\n",
       "      <td>benco</td>\n",
       "      <td>0.042</td>\n",
       "      <td>surgical</td>\n",
       "      <td>0.042</td>\n",
       "      <td>hsichealthcaredistributionmember</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>WAT</td>\n",
       "      <td>waters</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0001000697</td>\n",
       "      <td>0.401</td>\n",
       "      <td>ta</td>\n",
       "      <td>0.275</td>\n",
       "      <td>hplc</td>\n",
       "      <td>0.187</td>\n",
       "      <td>chromatography</td>\n",
       "      <td>...</td>\n",
       "      <td>micromass</td>\n",
       "      <td>0.045</td>\n",
       "      <td>20200627</td>\n",
       "      <td>0.044</td>\n",
       "      <td>analytical</td>\n",
       "      <td>0.043</td>\n",
       "      <td>milford</td>\n",
       "      <td>0.042</td>\n",
       "      <td>00114010</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ALGN</td>\n",
       "      <td>invisalign</td>\n",
       "      <td>0.812</td>\n",
       "      <td>aligners</td>\n",
       "      <td>0.224</td>\n",
       "      <td>ormco</td>\n",
       "      <td>0.159</td>\n",
       "      <td>aligner</td>\n",
       "      <td>0.159</td>\n",
       "      <td>dental</td>\n",
       "      <td>...</td>\n",
       "      <td>patient</td>\n",
       "      <td>0.059</td>\n",
       "      <td>clinical</td>\n",
       "      <td>0.055</td>\n",
       "      <td>cadent</td>\n",
       "      <td>0.054</td>\n",
       "      <td>teen</td>\n",
       "      <td>0.053</td>\n",
       "      <td>prescott</td>\n",
       "      <td>0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>EW</td>\n",
       "      <td>lifesciences</td>\n",
       "      <td>0.660</td>\n",
       "      <td>edwards</td>\n",
       "      <td>0.625</td>\n",
       "      <td>valve</td>\n",
       "      <td>0.241</td>\n",
       "      <td>heart</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0001099800</td>\n",
       "      <td>...</td>\n",
       "      <td>therapy</td>\n",
       "      <td>0.036</td>\n",
       "      <td>corevalve</td>\n",
       "      <td>0.030</td>\n",
       "      <td>manufacturing</td>\n",
       "      <td>0.029</td>\n",
       "      <td>perimount</td>\n",
       "      <td>0.028</td>\n",
       "      <td>patents</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tickers              word_0  tfidf_0           word_1  tfidf_1  \\\n",
       "0     AMZN           amazoncom    0.438            peacs    0.373   \n",
       "1      BBY              stores    0.641            store    0.378   \n",
       "2     BKNG               hotel    0.420     pricelinecom    0.404   \n",
       "3      MCD         restaurants    0.478         mcdonald    0.467   \n",
       "4     EBAY              paypal    0.819             ebay    0.423   \n",
       "5        F                ford    0.888       automotive    0.265   \n",
       "6       HD               depot    0.720           stores    0.440   \n",
       "7      TGT                card    0.582           stores    0.355   \n",
       "8      WHR           whirlpool    0.807           befiex    0.241   \n",
       "9      JPM            jpmorgan    0.539   lendingrelated    0.276   \n",
       "10    SIVB                 svb    0.885      noninterest    0.191   \n",
       "11     CFG       incmanagement    0.326            basel    0.269   \n",
       "12       C                citi    0.846        citigroup    0.349   \n",
       "13     ALL            allstate    0.723       0000899051    0.384   \n",
       "14     IVZ                 aum    0.661          invesco    0.621   \n",
       "15    ETFC              etrade    0.873       fourfamily    0.166   \n",
       "16     MET             metlife    0.794     policyholder    0.322   \n",
       "17     PFG            mortgage    0.530     accumulation    0.205   \n",
       "18    CBOE                cboe    0.895             bats    0.236   \n",
       "19     CTL         centurylink    0.713            qwest    0.320   \n",
       "20     IPG         interpublic    0.712          clients    0.266   \n",
       "21    NFLX           streaming    0.480              dvd    0.412   \n",
       "22    CHTR               cable    0.456              cco    0.390   \n",
       "23      FB            facebook    0.580             user    0.425   \n",
       "24    TWTR             twitter    0.602      advertisers    0.402   \n",
       "25    NWSA                 fox    0.415             news    0.375   \n",
       "26    FOXA          0001754301    0.645              fox    0.512   \n",
       "27     AMD                 amd    0.667               gf    0.355   \n",
       "28    INTC               intel    0.864  microprocessors    0.154   \n",
       "29    AAPL               apple    0.378             ipod    0.316   \n",
       "30    LRCX                 lam    0.592    semiconductor    0.440   \n",
       "31    MSFT           microsoft    0.559          windows    0.483   \n",
       "32    CTSH           cognizant    0.634           indian    0.383   \n",
       "33     ADS                card    0.524            miles    0.342   \n",
       "34      WU  consumertoconsumer    0.595     companynotes    0.343   \n",
       "35     ABT              abbott    0.932          abbotts    0.221   \n",
       "36     CVS            pharmacy    0.616              cvs    0.374   \n",
       "37     PFE              pfizer    0.573       0000078003    0.438   \n",
       "38     JNJ             johnson    0.560       0000200406    0.399   \n",
       "39    BIIB             tysabri    0.412           biogen    0.395   \n",
       "40    INCY            clinical    0.402             drug    0.392   \n",
       "41    HSIC              schein    0.670            henry    0.381   \n",
       "42     WAT              waters    0.723       0001000697    0.401   \n",
       "43    ALGN          invisalign    0.812         aligners    0.224   \n",
       "44      EW        lifesciences    0.660          edwards    0.625   \n",
       "\n",
       "               word_2  tfidf_2              word_3  tfidf_3          word_4  \\\n",
       "0         fulfillment    0.368              amazon    0.245        shipping   \n",
       "1                 sga    0.283           musicland    0.176     merchandise   \n",
       "2          bookingcom    0.347        reservations    0.273         airline   \n",
       "3          restaurant    0.383     companyoperated    0.348       mcdonalds   \n",
       "4               skype    0.144        marketplaces    0.106         paypals   \n",
       "5               motor    0.154            vehicles    0.133      incomeloss   \n",
       "6               store    0.298                  hd    0.167     merchandise   \n",
       "7              guests    0.254     comparablestore    0.248     merchandise   \n",
       "8           brazilian    0.193              maytag    0.186         indesit   \n",
       "9            mortgage    0.246                card    0.227           pages   \n",
       "10            clients    0.175              client    0.121         warrant   \n",
       "11               cbna    0.257                 frb    0.249             rbs   \n",
       "12              citis    0.145          citigroups    0.110        mortgage   \n",
       "13  propertyliability    0.225         reinsurance    0.225  contractholder   \n",
       "14               clos    0.279                 cip    0.225             clo   \n",
       "15          brokerage    0.153          0001015780    0.141           ajaxo   \n",
       "16        reinsurance    0.183                mlic    0.172      0001099219   \n",
       "17                pfg    0.199         residential    0.199       annuities   \n",
       "18                 c2    0.120             futures    0.112        clearing   \n",
       "19           00107784    0.309              embarq    0.242      centurytel   \n",
       "20                cmg    0.225                 ian    0.187         organic   \n",
       "21        subscribers    0.392             netflix    0.265            dvds   \n",
       "22             holdco    0.322                 cch    0.283             twc   \n",
       "23         0001326801    0.283                maus    0.256             ads   \n",
       "24               user    0.303                mdau    0.207          tweets   \n",
       "25                rea    0.337              foxtel    0.325            21st   \n",
       "26               21cf    0.423              sports    0.139     programming   \n",
       "27                fab    0.298            spansion    0.275        graphics   \n",
       "28         0000050863    0.141              memory    0.140            imft   \n",
       "29                mac    0.314              iphone    0.262       macintosh   \n",
       "30         0000707549    0.396                etch    0.215           wafer   \n",
       "31         0000789019    0.315                xbox    0.267          server   \n",
       "32         0001058290    0.295             clients    0.270           rupee   \n",
       "33                wfn    0.285                 air    0.221          reward   \n",
       "34            spinoff    0.240  consumertobusiness    0.212        speedpay   \n",
       "35       laboratories    0.181      pharmaceutical    0.097             tap   \n",
       "36           caremark    0.365          0000064803    0.263      pharmacies   \n",
       "37            generic    0.165                 fda    0.153         lipitor   \n",
       "38            janssen    0.289              patent    0.187  pharmaceutical   \n",
       "39            rituxan    0.372                idec    0.305          avonex   \n",
       "40             jakafi    0.387              trials    0.202   collaborators   \n",
       "41             dental    0.305          0001000228    0.284   practitioners   \n",
       "42                 ta    0.275                hplc    0.187  chromatography   \n",
       "43              ormco    0.159             aligner    0.159          dental   \n",
       "44              valve    0.241               heart    0.149      0001099800   \n",
       "\n",
       "    ...                    word_25 tfidf_25                word_26 tfidf_26  \\\n",
       "0   ...                      euros    0.059                   card    0.059   \n",
       "1   ...                largeformat    0.062              speakeasy    0.059   \n",
       "2   ...                     search    0.068               airlines    0.067   \n",
       "3   ...               conventional    0.033             franchisee    0.032   \n",
       "4   ...                    halfcom    0.035                    tpv    0.034   \n",
       "5   ...                     jaguar    0.033            nonconsumer    0.032   \n",
       "6   ...                      chain    0.045                  blake    0.042   \n",
       "7   ...                  minnesota    0.059                   gift    0.058   \n",
       "8   ...            forwardsoptions    0.046             oilrelated    0.044   \n",
       "9   ...                        msr    0.080                    afs    0.079   \n",
       "10  ...               riskweighted    0.039                standby    0.039   \n",
       "11  ...                    noncore    0.079                   msrs    0.079   \n",
       "12  ...                       vies    0.041                     fx    0.041   \n",
       "13  ...                       ibnr    0.038                    pif    0.037   \n",
       "14  ...               proportional    0.023               subgroup    0.022   \n",
       "15  ...                    lending    0.041         contentsetrade    0.039   \n",
       "16  ...                       rmbs    0.039           agricultural    0.036   \n",
       "17  ...                  actuarial    0.083           restructured    0.079   \n",
       "18  ...                marketmaker    0.034         exchangetraded    0.034   \n",
       "19  ...                    hosting    0.045                verizon    0.043   \n",
       "20  ...                     acxiom    0.058  reorganizationrelated    0.055   \n",
       "21  ...                        s1a    0.042                     tv    0.042   \n",
       "22  ...          subsidiariesnotes    0.068            franchising    0.065   \n",
       "23  ...                undesirable    0.039              geography    0.037   \n",
       "24  ...                     search    0.051                  video    0.049   \n",
       "25  ...                      jones    0.054                amplify    0.053   \n",
       "26  ...                   20201231    0.039                  mvpds    0.037   \n",
       "27  ...                    foundry    0.053                   atic    0.052   \n",
       "28  ...                         ec    0.049                   iotg    0.048   \n",
       "29  ...                   americas    0.065              education    0.064   \n",
       "30  ...                singlewafer    0.049               20180923    0.049   \n",
       "31  ...                      phone    0.051          subscriptions    0.050   \n",
       "32  ...                      souza    0.055                  visas    0.055   \n",
       "33  ...                   canadian    0.055               33360418    0.054   \n",
       "34  ...                      fexco    0.064            gainslosses    0.061   \n",
       "35  ...              manufacturing    0.018               generics    0.018   \n",
       "36  ...                    clients    0.049                    cms    0.044   \n",
       "37  ...  pfebiopharmasegmentmember    0.067          warnerlambert    0.066   \n",
       "38  ...               infringement    0.064                   teva    0.061   \n",
       "39  ...                  milestone    0.060                   drug    0.058   \n",
       "40  ...                  licensees    0.065          myelofibrosis    0.063   \n",
       "41  ...                    bergman    0.044              patterson    0.043   \n",
       "42  ...                  micromass    0.045               20200627    0.044   \n",
       "43  ...                    patient    0.059               clinical    0.055   \n",
       "44  ...                    therapy    0.036              corevalve    0.030   \n",
       "\n",
       "              word_27 tfidf_27           word_28 tfidf_28  \\\n",
       "0         wwwamazonca    0.056      wwwamazoncom    0.055   \n",
       "1            notebook    0.051          canadian    0.050   \n",
       "2                otcs    0.067           expedia    0.066   \n",
       "3               japan    0.032         breakfast    0.032   \n",
       "4             ebaycom    0.033            search    0.031   \n",
       "5                fcar    0.031       assetbacked    0.031   \n",
       "6              shrink    0.041          lighting    0.041   \n",
       "7             douglas    0.057               cvs    0.055   \n",
       "8           monetized    0.041          hotpoint    0.041   \n",
       "9                tier    0.078               rfs    0.077   \n",
       "10         reflective    0.038           volcker    0.036   \n",
       "11            lending    0.078               lcr    0.077   \n",
       "12     securitization    0.040       residential    0.040   \n",
       "13               mcca    0.035           spreads    0.035   \n",
       "14           sterling    0.022          amvescap    0.021   \n",
       "15            ltvcltv    0.036              etbh    0.036   \n",
       "16            surplus    0.034             alico    0.033   \n",
       "17   indemnifications    0.077              cmbs    0.076   \n",
       "18     multiplylisted    0.033      unrestricted    0.032   \n",
       "19           distance    0.043          carriers    0.042   \n",
       "20        uncommitted    0.052          philippe    0.051   \n",
       "21      entertainment    0.041         dvdbymail    0.041   \n",
       "22         indentures    0.065              viii    0.061   \n",
       "23                dau    0.035            patent    0.030   \n",
       "24      privatelyheld    0.047           pre2013    0.044   \n",
       "25              paytv    0.046       betterworse    0.043   \n",
       "26           20200701    0.037           heading    0.035   \n",
       "27               atmp    0.052            wafers    0.051   \n",
       "28            numonyx    0.047  corporationnotes    0.043   \n",
       "29       applebranded    0.062            patent    0.060   \n",
       "30             bullen    0.048         klatencor    0.048   \n",
       "31        intelligent    0.049     entertainment    0.049   \n",
       "32                itd    0.055            rupees    0.054   \n",
       "33               mile    0.054          merchant    0.051   \n",
       "34             mexico    0.060       crossborder    0.059   \n",
       "35             humira    0.017              drug    0.017   \n",
       "36         drugstores    0.043      minuteclinic    0.036   \n",
       "37                 rd    0.064           patents    0.059   \n",
       "38  usgaapnonusmember    0.059        biosimilar    0.058   \n",
       "39             trials    0.057        neurimmune    0.056   \n",
       "40          calithera    0.063             maxia    0.063   \n",
       "41              benco    0.042          surgical    0.042   \n",
       "42         analytical    0.043           milford    0.042   \n",
       "43             cadent    0.054              teen    0.053   \n",
       "44      manufacturing    0.029         perimount    0.028   \n",
       "\n",
       "                             word_29 tfidf_29  \n",
       "0                           unearned    0.054  \n",
       "1                        auctionrate    0.049  \n",
       "2                           braddock    0.065  \n",
       "3                            chicken    0.032  \n",
       "4                            auction    0.030  \n",
       "5                          commodity    0.030  \n",
       "6                               gift    0.038  \n",
       "7                                 3a    0.054  \n",
       "8                        compressors    0.040  \n",
       "9                            lending    0.076  \n",
       "10                        debentures    0.033  \n",
       "11                         servicing    0.067  \n",
       "12                               htm    0.037  \n",
       "13                          mortgage    0.035  \n",
       "14                    unconsolidated    0.021  \n",
       "15                        nonaccrual    0.036  \n",
       "16                               afs    0.033  \n",
       "17                 contentsprincipal    0.073  \n",
       "18                            outcry    0.031  \n",
       "19                             ilecs    0.041  \n",
       "20                      interpublics    0.049  \n",
       "21                          hastings    0.038  \n",
       "22                           comcast    0.059  \n",
       "23                          monetize    0.030  \n",
       "24                         333191552    0.042  \n",
       "25                         newsprint    0.043  \n",
       "26                               lot    0.033  \n",
       "27                        processors    0.049  \n",
       "28                           desktop    0.043  \n",
       "29                   microprocessors    0.059  \n",
       "30                              espp    0.047  \n",
       "31                           surface    0.048  \n",
       "32                              visa    0.053  \n",
       "33                         servicing    0.050  \n",
       "34                                fa    0.059  \n",
       "35                            xience    0.016  \n",
       "36                            payors    0.034  \n",
       "37                             drugs    0.058  \n",
       "38                             asrtm    0.057  \n",
       "39                           royalty    0.056  \n",
       "40                         candidate    0.062  \n",
       "41  hsichealthcaredistributionmember    0.041  \n",
       "42                          00114010    0.041  \n",
       "43                          prescott    0.049  \n",
       "44                           patents    0.028  \n",
       "\n",
       "[45 rows x 61 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top_k_word.to_csv(\"data/45_companies_top_30\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(pkl_path, 'token_counter.pkl'), 'rb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    c = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is where different from version 1, adding a beta term to calculate excess return.\n",
    "\"\"\"\n",
    "\n",
    "#preprocess the data and store them\n",
    "for ticker in all_data.keys():\n",
    "    \n",
    "    #give the cik for this ticker\n",
    "    cik = all_data[ticker]['cik']\n",
    "    \n",
    "    #goes into the directory to find the path for txtfiles\n",
    "    try:\n",
    "        files_10k = os.listdir(pathname_10k + cik + '/grabbed_text/')\n",
    "    except:\n",
    "        break\n",
    "    \n",
    "    #iterate through the 10k path and get the information and txt\n",
    "    for file_10k in files_10k:\n",
    "        \n",
    "        #get the release time\n",
    "        release_10k = file_10k[-14:-4]\n",
    "        \n",
    "        \"\"\"\n",
    "        adding beta = 1, denoting the market return value at that time to calculate excess return.\n",
    "        \"\"\"\n",
    "        \n",
    "        #get the 5 day excess return for the given release time\n",
    "        try:\n",
    "            excess_return = Get_Ex_Ret(ticker, release_10k)\n",
    "            \"\"\"\n",
    "            adding a market return term and subtracting it(assuming beta = 1)\n",
    "            \"\"\"\n",
    "            market_return = Get_Ex_Ret('SPY', release_10k)\n",
    "            excess_return = excess_return - market_return\n",
    "            \n",
    "        #exception may happen if we don't have the ticker name in yahoo finance, so we simply delete it    \n",
    "        except:\n",
    "            break\n",
    "    \n",
    "        #preprocess the txt and store it in a list of words\n",
    "        #open the text file and read file as lower string\n",
    "        with open(pathname_10k + cik + '/grabbed_text/' + file_10k, encoding = \"utf8\") as f:\n",
    "            string_temp = f.read().lower()\n",
    "        \n",
    "        #rule out all the stop words and store them in a list. Also rule out all the puctuations\n",
    "        filtered_words = [remove_punct(word) for word in string_temp.split() if word not in stop_words]\n",
    "        \n",
    "        #filter word for all words\n",
    "        filtered_words = list(filter(filter_words, filtered_words))\n",
    "        \n",
    "        #reduce word to its root form\n",
    "        filtered_words = [lemmatizer.lemmatize(word, pos = 'v') for word in filtered_words]\n",
    "        filtered_words = [lemmatizer.lemmatize(word, pos = 'n') for word in filtered_words]\n",
    "        \n",
    "        #remove all empty values\n",
    "        #while '' in filtered_words:\n",
    "            #filtered_words.remove('')\n",
    "        \n",
    "        #store all the information needed in the all_data dict\n",
    "        if filtered_words != []:\n",
    "            all_data[ticker]['10ks'][release_10k] = {}\n",
    "            all_data[ticker]['10ks'][release_10k]['ex_return'] = excess_return\n",
    "            all_data[ticker]['10ks'][release_10k]['words'] = ' '.join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is where different from version 1, adding a beta term to calculate excess return.\n",
    "\"\"\"\n",
    "\n",
    "for ticker in all_data.keys():\n",
    "    \n",
    "    #give the cik for this ticker\n",
    "    cik = all_data[ticker]['cik']\n",
    "    \n",
    "    #goes into the directory to find the path for txtfiles\n",
    "    try:\n",
    "        files_10q = os.listdir(pathname_10q + cik + '/grabbed_text/')\n",
    "    except:\n",
    "        break\n",
    "    \n",
    "    #iterate through the 10q path and get the information and txt\n",
    "    for file_10q in files_10q:\n",
    "        \n",
    "        #get the release time\n",
    "        release_10q = file_10q[-14:-4]       \n",
    "        \n",
    "        \"\"\"\n",
    "        adding beta = 1, denoting the market return value at that time to calculate excess return.\n",
    "        \"\"\"\n",
    "        \n",
    "        #get the 5 day excess return for the given release time\n",
    "        try:\n",
    "            excess_return = Get_Ex_Ret(ticker, release_10q)\n",
    "            \"\"\"\n",
    "            adding a market return term and subtracting it(assuming beta = 1)\n",
    "            \"\"\"\n",
    "            market_return = Get_Ex_Ret('SPY', release_10k)\n",
    "            excess_return = excess_return - market_return\n",
    "        \n",
    "        #exception may happen if we don't have the ticker name in yahoo finance, so we simply delete it    \n",
    "        except:\n",
    "            break\n",
    "    \n",
    "        #preprocess the txt and store it in a list of words\n",
    "        #open the text file and read file as lower string\n",
    "        with open(pathname_10q + cik + '/grabbed_text/' + file_10q, encoding = \"utf8\") as f:\n",
    "            string_temp = f.read().lower()\n",
    "        \n",
    "        #rule out all the stop words and store them in a list. Also rule out all the puctuations\n",
    "        filtered_words = [remove_punct(word) for word in string_temp.split() if word not in stop_words]\n",
    "        \n",
    "        #filter word for all words\n",
    "        filtered_words = list(filter(filter_words, filtered_words))\n",
    "        \n",
    "        #reduce word to its root form\n",
    "        filtered_words = [lemmatizer.lemmatize(word, pos = 'v') for word in filtered_words]\n",
    "        filtered_words = [lemmatizer.lemmatize(word, pos = 'n') for word in filtered_words]\n",
    "        \n",
    "        #remove all empty values\n",
    "        #while '' in filtered_words:\n",
    "            #filtered_words.remove('')\n",
    "        \n",
    "        #store all the information needed in the all_data dict\n",
    "        if filtered_words != []:\n",
    "            all_data[ticker]['10qs'][release_10q] = {}\n",
    "            all_data[ticker]['10qs'][release_10q]['ex_return'] = excess_return\n",
    "            all_data[ticker]['10qs'][release_10q]['words'] = ' '.join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### have a look at the data stucture now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data['NKE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze 10K word with tfidf and bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is to compute the tf values for the words in 10ks and collect the overall word list for computing idf in the next step\n",
    "\"\"\"\n",
    "#count the number of 10K documents we have\n",
    "document_num_10k = 0\n",
    "\n",
    "#word list for 10k \n",
    "word_list_10k = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10ks']:\n",
    "        \n",
    "        #we have document for a given date, so add 1 for document_num_10k\n",
    "        document_num_10k += 1\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        all_data[ticker]['10ks'][date]['tf'] = Counter(all_data[ticker]['10ks'][date]['words'].split())\n",
    "        \n",
    "        #iterate through the words in tf, which is the words of a given 10k document of a given date\n",
    "        for word in all_data[ticker]['10ks'][date]['tf']:\n",
    "            \n",
    "            #add one if it already contains the word, or add the this word to the dict if not\n",
    "            word_list_10k[word] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute idf value for the word in 10ks\n",
    "idf_10k = {}\n",
    "\n",
    "#iterate through all the words in word_list_10k\n",
    "for word in word_list_10k:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10k[word] = np.log(document_num_10k / (1 + word_list_10k[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#we already have the number of all 10k files\n",
    "# document_num_10k = document_num_10k\n",
    "\n",
    "#word pair list for 10k\n",
    "pair_list_10k = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10ks']:\n",
    "        \n",
    "        #there is no need for counting document_num_10k\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (2, 2))\n",
    "        \n",
    "        #fit it through vectorizer\n",
    "        fitted = vectorizer.fit_transform([all_data[ticker]['10ks'][date]['words']])\n",
    "        \n",
    "        #after vectorizer, we have feature name and feature count, feed them to a dataframe\n",
    "        df_temp = pd.DataFrame(index = vectorizer.get_feature_names(), data = np.squeeze(fitted.toarray()))\n",
    "#         #then we can add the data into the main data set all_data\n",
    "        all_data[ticker]['10ks'][date]['tf_pair'] = df_temp.to_dict()[0]\n",
    "#         print(all_data[ticker]['10ks'][date]['tf_pair'])\n",
    "        #print(all_data[ticker]['10ks'][date]['tf_pair'])\n",
    "        #iterate through the pairs in tf_pair, which is the words of a given 10k document of a given date\n",
    "        for pair in all_data[ticker]['10ks'][date]['tf_pair']:\n",
    "            \n",
    "            #add one if it already contains the pair, or add the this pair to the dict if not have\n",
    "            pair_list_10k[pair] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_list_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#compute idf value for the pair in 10ks\n",
    "idf_10k_pair = {}\n",
    "\n",
    "#iterate through all the pairs in pair_list_10k\n",
    "for pair in pair_list_10k:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10k_pair[pair] = np.log(document_num_10k / (1 + pair_list_10k[pair]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#we already have the number of all 10k files\n",
    "# document_num_10k = document_num_10k\n",
    "\n",
    "#word triple list for 10k\n",
    "triple_list_10k = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10ks']:\n",
    "        \n",
    "        #there is no need for counting document_num_10k\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (3, 3))\n",
    "        \n",
    "        #fit it through vectorizer\n",
    "        fitted = vectorizer.fit_transform([all_data[ticker]['10ks'][date]['words']])\n",
    "        \n",
    "        #after vectorizer, we have feature name and feature count, feed them to a dataframe\n",
    "        df_temp = pd.DataFrame(index = vectorizer.get_feature_names(), data = np.squeeze(fitted.toarray()))\n",
    "        \n",
    "        #then we can add the data into the main data set all_data\n",
    "        all_data[ticker]['10ks'][date]['tf_triple'] = df_temp.to_dict()[0]\n",
    "        \n",
    "        #iterate through the triples in tf_triple, which is the words of a given 10k document of a given date\n",
    "        for triple in all_data[ticker]['10ks'][date]['tf_triple']:\n",
    "            \n",
    "            #add one if it already contains the triple, or add the this triple to the dict if not have\n",
    "            triple_list_10k[triple] += 1\n",
    "            \n",
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#compute idf value for the triple in 10ks\n",
    "idf_10k_triple = {}\n",
    "\n",
    "#iterate through all the triples in triple_list_10k\n",
    "for triple in triple_list_10k:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10k_triple[triple] = np.log(document_num_10k / (1 + triple_list_10k[triple]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doing the same to 10qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also do the same to 10q files\n",
    "\n",
    "\"\"\"\n",
    "This part is to compute the tf values for the words in 10qs and collect the overall word list for computing idf in the next step\n",
    "\"\"\"\n",
    "#count the number of 10Q documents we have\n",
    "document_num_10q = 0\n",
    "\n",
    "#word list for 10q \n",
    "word_list_10q = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10qs']:\n",
    "        \n",
    "        #we have document for a given date, so add 1 for document_num_10q\n",
    "        document_num_10q += 1\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        all_data[ticker]['10qs'][date]['tf'] = Counter(all_data[ticker]['10qs'][date]['words'].split())\n",
    "        \n",
    "        #iterate through the words in tf, which is the words of a given 10q document of a given date\n",
    "        for word in all_data[ticker]['10qs'][date]['tf']:\n",
    "            \n",
    "            #add one if it already contains the word, or add the this word to the dict if not\n",
    "            word_list_10q[word] += 1\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute idf value for the word in 10qs\n",
    "idf_10q = {}\n",
    "\n",
    "#iterate through all the words in word_list_10q\n",
    "for word in word_list_10q:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10q[word] = np.log(document_num_10q / (1 + word_list_10q[word]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#we already have the number of all 10q files\n",
    "# document_num_10q = document_num_10q\n",
    "\n",
    "#word pair list for 10q\n",
    "pair_list_10q = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10qs']:\n",
    "        \n",
    "        #there is no need for counting document_num_10q\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (2, 2))\n",
    "        \n",
    "        #fit it through vectorizer\n",
    "        fitted = vectorizer.fit_transform([all_data[ticker]['10qs'][date]['words']])\n",
    "        \n",
    "        #after vectorizer, we have feature name and feature count, feed them to a dataframe\n",
    "        df_temp = pd.DataFrame(index = vectorizer.get_feature_names(), data = np.squeeze(fitted.toarray()))\n",
    "        \n",
    "        #then we can add the data into the main data set all_data\n",
    "        all_data[ticker]['10qs'][date]['tf_pair'] = df_temp.to_dict()[0]\n",
    "        \n",
    "        #iterate through the pairs in tf_pair, which is the words of a given 10q document of a given date\n",
    "        for pair in all_data[ticker]['10qs'][date]['tf_pair']:\n",
    "            \n",
    "            #add one if it already contains the pair, or add the this pair to the dict if not have\n",
    "            pair_list_10q[pair] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#compute idf value for the pair in 10qs\n",
    "idf_10q_pair = {}\n",
    "\n",
    "#iterate through all the pairs in pair_list_10q\n",
    "for pair in pair_list_10q:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10q_pair[pair] = np.log(document_num_10q / (1 + pair_list_10q[pair]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#we already have the number of all 10q files\n",
    "# document_num_10q = document_num_10q\n",
    "\n",
    "#word triple list for 10q\n",
    "triple_list_10q = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10qs']:\n",
    "        \n",
    "        #there is no need for counting document_num_10q\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        \"\"\"\n",
    "        The only adaptation from pair to triple is changing ngram_range, needing futher simplification of code\n",
    "        \"\"\"\n",
    "        vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (3, 3))\n",
    "        \n",
    "        #fit it through vectorizer\n",
    "        fitted = vectorizer.fit_transform([all_data[ticker]['10qs'][date]['words']])\n",
    "        \n",
    "        #after vectorizer, we have feature name and feature count, feed them to a dataframe\n",
    "        df_temp = pd.DataFrame(index = vectorizer.get_feature_names(), data = np.squeeze(fitted.toarray()))\n",
    "        \n",
    "        #then we can add the data into the main data set all_data\n",
    "        all_data[ticker]['10qs'][date]['tf_triple'] = df_temp.to_dict()[0]\n",
    "        \n",
    "        #iterate through the triples in tf_triple, which is the words of a given 10q document of a given date\n",
    "        for triple in all_data[ticker]['10qs'][date]['tf_triple']:\n",
    "            \n",
    "            #add one if it already contains the triple, or add the this triple to the dict if not have\n",
    "            triple_list_10q[triple] += 1\n",
    "            \n",
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#compute idf value for the triple in 10qs\n",
    "idf_10q_triple = {}\n",
    "\n",
    "#iterate through all the triples in triple_list_10q\n",
    "for triple in triple_list_10q:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10q_triple[triple] = np.log(document_num_10q / (1 + triple_list_10q[triple]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### have a look at the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is new in version 2, containing idfs for word pairs\n",
    "\"\"\"\n",
    "\n",
    "idf_10q_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is new in version 2, containing idfs for word pairs\n",
    "\"\"\"\n",
    "\n",
    "idf_10q_triple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the data for future use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this part is for storing the data for future use\n",
    "\"\"\"\n",
    "#delete word in all_data for storage\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10qs']:\n",
    "        \n",
    "        del all_data[ticker]['10qs'][date]['words']\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10ks']:\n",
    "        \n",
    "        del all_data[ticker]['10ks'][date]['words']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write all_data to a json file\n",
    "with open('all_data.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(all_data))\n",
    "\n",
    "#write idf_10k to a json file\n",
    "with open('idf_10k.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10k))\n",
    "\n",
    "#write idf_10q to a json file    \n",
    "with open('idf_10q.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10q))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is new in version 2, storing files for word pairs and word triples\n",
    "\"\"\"\n",
    "#write idf_10k_pair to a json file  \n",
    "with open('idf_10k_pair.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10k_pair))\n",
    "\n",
    "#write idf_10q_pair to a json file    \n",
    "with open('idf_10q_pair.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10q_pair))\n",
    "    \n",
    "#write idf_10k_triple to a json file  \n",
    "with open('idf_10k_triple.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10k_triple))\n",
    "\n",
    "#write idf_10q_triple to a json file    \n",
    "with open('idf_10q_triple.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10q_triple))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
